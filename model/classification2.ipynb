{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e351d2b",
   "metadata": {},
   "source": [
    "# Model Logic (Return-Based Direction Classification)\n",
    "\n",
    "This project predicts **whether NVIDIA’s closing price will go *UP* or *DOWN* on day *t*** using only information available at the end of day *(t–1)*.  \n",
    "This ensures the model never sees future data and respects real-world trading constraints.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What the Model Uses\n",
    "\n",
    "For each day *(t–1)*, we use:\n",
    "\n",
    "- **TF-IDF features of all news headlines from day (t–1)**\n",
    "- **Return(t–1)**, defined as:\n",
    "\n",
    "$$\n",
    "\\text{return}(t-1) = \\frac{\\text{close}(t-1) - \\text{close}(t-2)}{\\text{close}(t-2)}\n",
    "$$\n",
    "\n",
    "These represent all market + news information available before predicting day *t*.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. What the Model Predicts\n",
    "\n",
    "We predict the **direction** of the next day's price movement:\n",
    "\n",
    "$$\n",
    "\\text{Movement}(t) =\n",
    "\\begin{cases}\n",
    "1 & \\text{if return}(t) > 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This makes the task a **binary classification**:\n",
    "\n",
    "- **1 → stock goes up**  \n",
    "- **0 → stock goes down or stays flat**\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Time-Series Alignment\n",
    "\n",
    "Because returns describe the *change* between two days, inputs and outputs shift:\n",
    "\n",
    "| Actual Day | TF-IDF Used | Return Used | Predict Label |\n",
    "|------------|-------------|-------------|----------------|\n",
    "| t = 1 | TF-IDF(1) | Return(1) | Movement(2) |\n",
    "| t = 2 | TF-IDF(2) | Return(2) | Movement(3) |\n",
    "| t = 3 | TF-IDF(3) | Return(3) | Movement(4) |\n",
    "\n",
    "Each training sample is aligned as:\n",
    "\n",
    "**Features:** TF-IDF(t–1), Return(t–1)  \n",
    "**Target:** Movement(t)\n",
    "\n",
    "This alignment results in **N–2 valid training samples**.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Example of Alignment\n",
    "\n",
    "Suppose we have:\n",
    "\n",
    "| Day | TF-IDF | Close |\n",
    "|-----|--------|--------|\n",
    "| 1 | `[0.2, 0.1]` | 100 |\n",
    "| 2 | `[0.5, 0.0]` | 102 |\n",
    "| 3 | `[0.3, 0.2]` | 101 |\n",
    "| 4 | `[0.1, 0.4]` | 103 |\n",
    "\n",
    "Returns:\n",
    "\n",
    "| Day | Return |\n",
    "|-----|--------|\n",
    "| 2 | (102−100)/100 = 0.02 |\n",
    "| 3 | (101−102)/102 = −0.0098 |\n",
    "| 4 | (103−101)/101 = 0.0198 |\n",
    "\n",
    "Aligned training rows:\n",
    "\n",
    "| Row | TF-IDF(t–1) | Return(t–1) | Predict Movement(t) |\n",
    "|-----|-------------|--------------|---------------------|\n",
    "| 0 | `[0.2, 0.1]` | 0.02 | Movement(2)=1 |\n",
    "| 1 | `[0.5, 0.0]` | -0.0098 | Movement(3)=0 |\n",
    "| 2 | `[0.3, 0.2]` | 0.0198 | Movement(4)=1 |\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Modeling Pipeline\n",
    "\n",
    "The full pipeline:\n",
    "\n",
    "1. **Reduce TF-IDF dimensionality** using Truncated SVD (30 or 50 components).  \n",
    "2. **Normalize the previous-day return** using StandardScaler.  \n",
    "3. **Train a Logistic Regression classifier**, tuning penalty and C using GridSearchCV.  \n",
    "4. **Perform walk-forward prediction** on each test day, always using actual return(t–1).  \n",
    "5. **Sweep SVD dimensions and decision thresholds** to identify the strongest model.\n",
    "\n",
    "This produces a fully time-aligned, feature-engineered classifier for predicting NVIDIA’s next-day price direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6109288d",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fe58c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import joblib\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad79386",
   "metadata": {},
   "source": [
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbc69309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "DATA_PATH = Path(\"../data/\")\n",
    "DATA_OUTPUT_PATH = Path(\"../output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb3d876c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>sourcecountry</th>\n",
       "      <th>seendate</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>domain</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>Australia</td>\n",
       "      <td>2024-11-18 03:45:00+00:00</td>\n",
       "      <td>2024-11-18</td>\n",
       "      <td>https://www.fool.com.au/2024/11/18/prediction-...</td>\n",
       "      <td>Prediction : Nvidia stock is going to soar aft...</td>\n",
       "      <td>fool.com.au</td>\n",
       "      <td>139.5</td>\n",
       "      <td>141.55</td>\n",
       "      <td>137.15</td>\n",
       "      <td>140.15</td>\n",
       "      <td>140.11</td>\n",
       "      <td>221205300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>English</td>\n",
       "      <td>Cyprus</td>\n",
       "      <td>2024-11-18 04:00:00+00:00</td>\n",
       "      <td>2024-11-18</td>\n",
       "      <td>https://cyprus-mail.com/2024/11/18/softbank-fi...</td>\n",
       "      <td>SoftBank first to receive new Nvidia chips for...</td>\n",
       "      <td>cyprus-mail.com</td>\n",
       "      <td>139.5</td>\n",
       "      <td>141.55</td>\n",
       "      <td>137.15</td>\n",
       "      <td>140.15</td>\n",
       "      <td>140.11</td>\n",
       "      <td>221205300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>China</td>\n",
       "      <td>2024-11-18 04:00:00+00:00</td>\n",
       "      <td>2024-11-18</td>\n",
       "      <td>https://www.morningstar.com/markets/this-unlov...</td>\n",
       "      <td>Why Small - Cap Value Stocks Look Attractive R...</td>\n",
       "      <td>morningstar.com</td>\n",
       "      <td>139.5</td>\n",
       "      <td>141.55</td>\n",
       "      <td>137.15</td>\n",
       "      <td>140.15</td>\n",
       "      <td>140.11</td>\n",
       "      <td>221205300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>English</td>\n",
       "      <td>United States</td>\n",
       "      <td>2024-11-18 06:30:00+00:00</td>\n",
       "      <td>2024-11-18</td>\n",
       "      <td>https://247wallst.com/market-news/2024/11/17/n...</td>\n",
       "      <td>Nasdaq Futures Up Sunday Night : NVIDIA Earnin...</td>\n",
       "      <td>247wallst.com</td>\n",
       "      <td>139.5</td>\n",
       "      <td>141.55</td>\n",
       "      <td>137.15</td>\n",
       "      <td>140.15</td>\n",
       "      <td>140.11</td>\n",
       "      <td>221205300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>English</td>\n",
       "      <td>United States</td>\n",
       "      <td>2024-11-18 11:00:00+00:00</td>\n",
       "      <td>2024-11-18</td>\n",
       "      <td>https://www.benzinga.com/24/11/42029943/dow-tu...</td>\n",
       "      <td>Dow Tumbles Over 300 Points Following Economic...</td>\n",
       "      <td>benzinga.com</td>\n",
       "      <td>139.5</td>\n",
       "      <td>141.55</td>\n",
       "      <td>137.15</td>\n",
       "      <td>140.15</td>\n",
       "      <td>140.11</td>\n",
       "      <td>221205300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language  sourcecountry                   seendate       date  \\\n",
       "0  English      Australia  2024-11-18 03:45:00+00:00 2024-11-18   \n",
       "1  English         Cyprus  2024-11-18 04:00:00+00:00 2024-11-18   \n",
       "2  English          China  2024-11-18 04:00:00+00:00 2024-11-18   \n",
       "3  English  United States  2024-11-18 06:30:00+00:00 2024-11-18   \n",
       "4  English  United States  2024-11-18 11:00:00+00:00 2024-11-18   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.fool.com.au/2024/11/18/prediction-...   \n",
       "1  https://cyprus-mail.com/2024/11/18/softbank-fi...   \n",
       "2  https://www.morningstar.com/markets/this-unlov...   \n",
       "3  https://247wallst.com/market-news/2024/11/17/n...   \n",
       "4  https://www.benzinga.com/24/11/42029943/dow-tu...   \n",
       "\n",
       "                                               title           domain   open  \\\n",
       "0  Prediction : Nvidia stock is going to soar aft...      fool.com.au  139.5   \n",
       "1  SoftBank first to receive new Nvidia chips for...  cyprus-mail.com  139.5   \n",
       "2  Why Small - Cap Value Stocks Look Attractive R...  morningstar.com  139.5   \n",
       "3  Nasdaq Futures Up Sunday Night : NVIDIA Earnin...    247wallst.com  139.5   \n",
       "4  Dow Tumbles Over 300 Points Following Economic...     benzinga.com  139.5   \n",
       "\n",
       "     high     low   close  adj_close     volume  \n",
       "0  141.55  137.15  140.15     140.11  221205300  \n",
       "1  141.55  137.15  140.15     140.11  221205300  \n",
       "2  141.55  137.15  140.15     140.11  221205300  \n",
       "3  141.55  137.15  140.15     140.11  221205300  \n",
       "4  141.55  137.15  140.15     140.11  221205300  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load tfidf matrices\n",
    "X_train_text = sparse.load_npz(f\"{DATA_PATH}/tfidf/X_train_tfidf.npz\")\n",
    "X_test_text  = sparse.load_npz(f\"{DATA_PATH}/tfidf/X_test_tfidf.npz\")\n",
    "vectorizer   = joblib.load(f\"{DATA_PATH}/tfidf/tfidf_vectorizer.pkl\")\n",
    "\n",
    "df_nvidia = pd.read_csv(DATA_PATH / \"NVIDIA_Merged_20241101-Present.csv\")\n",
    "df_nvidia['date'] = pd.to_datetime(df_nvidia['date'])\n",
    "df_nvidia.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de253633",
   "metadata": {},
   "source": [
    "# 3. Train/Test Split (Daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a76f475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train days: 226\n",
      "Test days : 15\n",
      "TF-IDF train shape: (226, 50)\n",
      "TF-IDF test shape : (15, 50)\n"
     ]
    }
   ],
   "source": [
    "SPLIT_DATE = pd.Timestamp(\"2025-11-01\")\n",
    "\n",
    "train_df = df_nvidia[df_nvidia[\"date\"] < SPLIT_DATE]\n",
    "test_df  = df_nvidia[df_nvidia[\"date\"] >= SPLIT_DATE]\n",
    "\n",
    "train_daily = (\n",
    "    train_df.groupby(\"date\")[\"close\"]\n",
    "    .first()\n",
    "    .reset_index()\n",
    "    .sort_values(\"date\")\n",
    ")\n",
    "\n",
    "test_daily = (\n",
    "    test_df.groupby(\"date\")[\"close\"]\n",
    "    .first()\n",
    "    .reset_index()\n",
    "    .sort_values(\"date\")\n",
    ")\n",
    "\n",
    "train_dates = train_daily[\"date\"].values\n",
    "test_dates  = test_daily[\"date\"].values\n",
    "\n",
    "y_train_all = train_daily[\"close\"].values\n",
    "y_test_all  = test_daily[\"close\"].values\n",
    "\n",
    "print(\"Train days:\", len(train_dates))\n",
    "print(\"Test days :\", len(test_dates))\n",
    "print(\"TF-IDF train shape:\", X_train_text.shape)\n",
    "print(\"TF-IDF test shape :\", X_test_text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127bb3d8",
   "metadata": {},
   "source": [
    "# 4. Compute Returns & Time-Series Alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "623a9e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_returns = (y_train_all[1:] - y_train_all[:-1]) / y_train_all[:-1]   # length N-1\n",
    "test_returns  = (y_test_all[1:] - y_test_all[:-1]) / y_test_all[:-1]      # length M-1\n",
    "\n",
    "first_test_return_prev = (y_test_all[0] - y_train_all[-1]) / y_train_all[-1]\n",
    "test_returns = np.r_[first_test_return_prev, test_returns]  # length M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c85dc21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF aligned: (224, 50)\n",
      "Returns aligned: (224, 1)\n",
      "Labels aligned : (224,)\n"
     ]
    }
   ],
   "source": [
    "# Labels: movement(t)\n",
    "y_train = (train_returns[1:] > 0).astype(int)            # length N-2\n",
    "\n",
    "# Feature: return(t-1)\n",
    "prev_return_train = train_returns[:-1].reshape(-1, 1)     # length N-2\n",
    "\n",
    "# Feature: TF-IDF(t-1)\n",
    "X_train_tfidf_prev = X_train_text[:-2]                    # length N-2\n",
    "\n",
    "print(\"TF-IDF aligned:\", X_train_tfidf_prev.shape)\n",
    "print(\"Returns aligned:\", prev_return_train.shape)\n",
    "print(\"Labels aligned :\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d172f221",
   "metadata": {},
   "source": [
    "# 5. GridSearch for Optimal Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7f2eddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training SVD=30 ===\n",
      "\n",
      "=== Training SVD=50 ===\n"
     ]
    }
   ],
   "source": [
    "svd_dims = [30, 50]\n",
    "thresholds = np.arange(0.4, 0.6, 0.025)\n",
    "\n",
    "results = []\n",
    "\n",
    "for dim in svd_dims:\n",
    "    print(f\"\\n=== Training SVD={dim} ===\")\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=dim, random_state=42)\n",
    "    X_train_svd = svd.fit_transform(X_train_tfidf_prev)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    prev_return_scaled = scaler.fit_transform(prev_return_train)\n",
    "    \n",
    "    X_train_iter = np.hstack([X_train_svd, prev_return_scaled])\n",
    "    \n",
    "    # hyperparameter search\n",
    "    param_grid = {\n",
    "        \"penalty\": [\"l1\", \"l2\"],\n",
    "        \"C\": [0.01, 0.1, 1, 5, 10],\n",
    "        \"solver\": [\"saga\"],\n",
    "        \"max_iter\": [2000],\n",
    "    }\n",
    "    \n",
    "    logreg = LogisticRegression(class_weight=\"balanced\")\n",
    "    \n",
    "    grid = GridSearchCV(\n",
    "        logreg, param_grid, scoring=\"f1\", cv=5, n_jobs=-1, verbose=0\n",
    "    )\n",
    "    grid.fit(X_train_iter, y_train)\n",
    "    \n",
    "    best_model = grid.best_estimator_\n",
    "    \n",
    "    # prediction\n",
    "    X_test_svd = svd.transform(X_test_text)\n",
    "    \n",
    "    predicted_probs = []\n",
    "    prev_ret = train_returns[-1]\n",
    "    \n",
    "    for i in range(len(test_dates)):\n",
    "        prev_scaled = scaler.transform([[prev_ret]])\n",
    "        X_i = np.hstack([X_test_svd[i].reshape(1, -1), prev_scaled])\n",
    "        \n",
    "        prob_up = best_model.predict_proba(X_i)[0][1]\n",
    "        predicted_probs.append(prob_up)\n",
    "        \n",
    "        prev_ret = test_returns[i]\n",
    "    \n",
    "    actual = (test_returns > 0).astype(int)\n",
    "    \n",
    "    # evaluate thresholds\n",
    "    for thr in thresholds:\n",
    "        preds = (np.array(predicted_probs) >= thr).astype(int)\n",
    "        \n",
    "        acc = accuracy_score(actual, preds)\n",
    "        prec = precision_score(actual, preds, zero_division=0)\n",
    "        rec  = recall_score(actual, preds)\n",
    "        f1   = f1_score(actual, preds)\n",
    "        \n",
    "        results.append({\n",
    "            \"svd_dim\": dim,\n",
    "            \"threshold\": thr,\n",
    "            \"accuracy\": acc,\n",
    "            \"precision\": prec,\n",
    "            \"recall\": rec,\n",
    "            \"f1\": f1,\n",
    "            \"confusion\": confusion_matrix(actual, preds)\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d3226",
   "metadata": {},
   "source": [
    "# 6. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f81efa7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>svd_dim</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>confusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>[[5, 4], [0, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>50</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>[[5, 4], [0, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>[[8, 1], [2, 4]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>50</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>[[8, 1], [2, 4]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>[[9, 0], [3, 3]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>50</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>[[9, 0], [3, 3]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>[[1, 8], [0, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>50</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>[[1, 8], [0, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>[[0, 9], [0, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>50</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>[[0, 9], [0, 6]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    svd_dim  threshold  accuracy  precision    recall        f1  \\\n",
       "2        30      0.450  0.733333   0.600000  1.000000  0.750000   \n",
       "10       50      0.450  0.733333   0.600000  1.000000  0.750000   \n",
       "3        30      0.475  0.800000   0.800000  0.666667  0.727273   \n",
       "11       50      0.475  0.800000   0.800000  0.666667  0.727273   \n",
       "4        30      0.500  0.800000   1.000000  0.500000  0.666667   \n",
       "12       50      0.500  0.800000   1.000000  0.500000  0.666667   \n",
       "1        30      0.425  0.466667   0.428571  1.000000  0.600000   \n",
       "9        50      0.425  0.466667   0.428571  1.000000  0.600000   \n",
       "0        30      0.400  0.400000   0.400000  1.000000  0.571429   \n",
       "8        50      0.400  0.400000   0.400000  1.000000  0.571429   \n",
       "\n",
       "           confusion  \n",
       "2   [[5, 4], [0, 6]]  \n",
       "10  [[5, 4], [0, 6]]  \n",
       "3   [[8, 1], [2, 4]]  \n",
       "11  [[8, 1], [2, 4]]  \n",
       "4   [[9, 0], [3, 3]]  \n",
       "12  [[9, 0], [3, 3]]  \n",
       "1   [[1, 8], [0, 6]]  \n",
       "9   [[1, 8], [0, 6]]  \n",
       "0   [[0, 9], [0, 6]]  \n",
       "8   [[0, 9], [0, 6]]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df_sorted = results_df.sort_values(\"f1\", ascending=False)\n",
    "\n",
    "results_df_sorted.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7920cb7",
   "metadata": {},
   "source": [
    "The grid search identified a clear optimal configuration for predicting NVIDIA's next-day stock direction. The best model used 30 SVD components (or 50, given performance was equivalent) and a classification threshold of 0.45, achieving an f1 score of 0.75. This configuration yielded:\n",
    "- 100% recall, meaning the model successfully identified all upward price movements\n",
    "- 60% precision, indicating some false positives but acceptable given the strong recall\n",
    "- Overall accuracy of 73.33%, performing better than random guessing (50%)\n",
    "\n",
    "Some general patterns from the results:\n",
    "- Threshold tuning plays a larger role in the model performance than SVD dimensionality of the tfidf vectors, given that both 30 and 50 tfidf components preserved identical amounts of semantic information from article headlines (only changes precision-recall balance)\n",
    "- At lower thresholds (0.40-0.425), the model becomes over-aggressive and predicts 'up' on almost every day, which inflates recall and harms precision\n",
    "- At higher thresholds (0.475-0.50), the model becomes too conservative and misses many upward movements\n",
    "- The models tend to perform better at identifying 'up' movements than 'down' movements, which is a realistic pattern within the financial market given market asymmetry and the positive skew of headline sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2848565",
   "metadata": {},
   "source": [
    "# 7. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d3eeac",
   "metadata": {},
   "source": [
    "The results suggest that daily financial headlines contain meaningful predictive signals about NVIDIA's next-day stock direction, especially when combined with the previous day's return. Even a simple linear classifier like logistic regression with L1 regularization can extract enough information from the tfidf values and returns to achieve decent directional accuracy and strong recall.\n",
    "\n",
    "However, the model's predictive strength is asymmetric, given it is highly sensitive to upward momentum (good recall) but is less reliable at identifying downward movements (occasional false positives). This asymmetry aligns with known properties of financial news, namely, headlines often express positive or optimistic sentiment through bias, and downward market moves are harder to anticipate and might be driven by external shocks that are not captured in daily news. Again, given the similarity between the 30-component and 50-component SVD of the tfidf embeddings, this suggests the tfidf embeddings contain relatively low-rank signals, and increasing embedding dimensionality does not meaningfully improve the predictive power of our model.\n",
    "\n",
    "Some potential next steps we could take are:\n",
    "- Adding sentiment scores using VADER or FinBERT alongside tfidf\n",
    "- Incorporate headline volume (# of articles per day)\n",
    "- Multi-day history features, like 3-7 day rolling averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32cb38b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "buan5312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
