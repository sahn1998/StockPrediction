{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e351d2b",
   "metadata": {},
   "source": [
    "# Model Logic (Return-Based Direction Classification Using Logistic Regression)\n",
    "\n",
    "This project predicts **whether NVIDIA’s closing price will go *UP* or *DOWN* on day *t*** using only information available at the end of day *(t–1)*.  \n",
    "This ensures the model never sees future data and respects real-world trading constraints.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What the Model Uses\n",
    "\n",
    "For each day *(t–1)*, we use:\n",
    "\n",
    "- **TF-IDF features of all news headlines from day (t–1)**\n",
    "- **Return(t–1)**, defined as:\n",
    "\n",
    "$$\n",
    "\\text{return}(t-1) = \\frac{\\text{close}(t-1) - \\text{close}(t-2)}{\\text{close}(t-2)}\n",
    "$$\n",
    "\n",
    "These represent all market + news information available before predicting day *t*.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. What the Model Predicts\n",
    "\n",
    "We predict the **direction** of the next day's price movement:\n",
    "\n",
    "$$\n",
    "\\text{Movement}(t) =\n",
    "\\begin{cases}\n",
    "1 & \\text{if return}(t) > 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This makes the task a **binary classification**:\n",
    "\n",
    "- **1 → stock goes up**  \n",
    "- **0 → stock goes down or stays flat**\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Time-Series Alignment\n",
    "\n",
    "Because returns describe the *change* between two days, inputs and outputs shift:\n",
    "\n",
    "| Actual Day | TF-IDF Used | Return Used | Predict Label |\n",
    "|------------|-------------|-------------|----------------|\n",
    "| t = 1 | TF-IDF(1) | Return(1) | Movement(2) |\n",
    "| t = 2 | TF-IDF(2) | Return(2) | Movement(3) |\n",
    "| t = 3 | TF-IDF(3) | Return(3) | Movement(4) |\n",
    "\n",
    "Each training sample is aligned as:\n",
    "\n",
    "**Features:** TF-IDF(t–1), Return(t–1)  \n",
    "**Target:** Movement(t)\n",
    "\n",
    "This alignment results in **N–2 valid training samples**.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Example of Alignment\n",
    "\n",
    "Suppose we have:\n",
    "\n",
    "| Day | TF-IDF | Close |\n",
    "|-----|--------|--------|\n",
    "| 1 | `[0.2, 0.1]` | 100 |\n",
    "| 2 | `[0.5, 0.0]` | 102 |\n",
    "| 3 | `[0.3, 0.2]` | 101 |\n",
    "| 4 | `[0.1, 0.4]` | 103 |\n",
    "\n",
    "Returns:\n",
    "\n",
    "| Day | Return |\n",
    "|-----|--------|\n",
    "| 2 | (102−100)/100 = 0.02 |\n",
    "| 3 | (101−102)/102 = −0.0098 |\n",
    "| 4 | (103−101)/101 = 0.0198 |\n",
    "\n",
    "Aligned training rows:\n",
    "\n",
    "| Row | TF-IDF(t–1) | Return(t–1) | Predict Movement(t) |\n",
    "|-----|-------------|--------------|---------------------|\n",
    "| 0 | `[0.2, 0.1]` | 0.02 | Movement(2)=1 |\n",
    "| 1 | `[0.5, 0.0]` | -0.0098 | Movement(3)=0 |\n",
    "| 2 | `[0.3, 0.2]` | 0.0198 | Movement(4)=1 |\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Modeling Pipeline\n",
    "\n",
    "The full pipeline:\n",
    "\n",
    "1. **Reduce TF-IDF dimensionality** using Truncated SVD (30 or 50 components).  \n",
    "2. **Normalize the previous-day return** using StandardScaler.  \n",
    "3. **Train a Logistic Regression classifier**, tuning penalty and C using GridSearchCV.  \n",
    "4. **Perform walk-forward prediction** on each test day, always using actual return(t–1).  \n",
    "5. **Sweep SVD dimensions and decision thresholds** to identify the strongest model.\n",
    "\n",
    "This produces a fully time-aligned, feature-engineered classifier for predicting NVIDIA’s next-day price direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6109288d",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fe58c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import joblib\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad79386",
   "metadata": {},
   "source": [
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbc69309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "DATA_PATH = Path(\"../data/\")\n",
    "DATA_OUTPUT_PATH = Path(\"../output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb3d876c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>sourcecountry</th>\n",
       "      <th>seendate</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>domain</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>Australia</td>\n",
       "      <td>2024-11-18 03:45:00+00:00</td>\n",
       "      <td>2024-11-18</td>\n",
       "      <td>https://www.fool.com.au/2024/11/18/prediction-...</td>\n",
       "      <td>Prediction : Nvidia stock is going to soar aft...</td>\n",
       "      <td>fool.com.au</td>\n",
       "      <td>139.5</td>\n",
       "      <td>141.55</td>\n",
       "      <td>137.15</td>\n",
       "      <td>140.15</td>\n",
       "      <td>140.11</td>\n",
       "      <td>221205300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>English</td>\n",
       "      <td>Cyprus</td>\n",
       "      <td>2024-11-18 04:00:00+00:00</td>\n",
       "      <td>2024-11-18</td>\n",
       "      <td>https://cyprus-mail.com/2024/11/18/softbank-fi...</td>\n",
       "      <td>SoftBank first to receive new Nvidia chips for...</td>\n",
       "      <td>cyprus-mail.com</td>\n",
       "      <td>139.5</td>\n",
       "      <td>141.55</td>\n",
       "      <td>137.15</td>\n",
       "      <td>140.15</td>\n",
       "      <td>140.11</td>\n",
       "      <td>221205300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>China</td>\n",
       "      <td>2024-11-18 04:00:00+00:00</td>\n",
       "      <td>2024-11-18</td>\n",
       "      <td>https://www.morningstar.com/markets/this-unlov...</td>\n",
       "      <td>Why Small - Cap Value Stocks Look Attractive R...</td>\n",
       "      <td>morningstar.com</td>\n",
       "      <td>139.5</td>\n",
       "      <td>141.55</td>\n",
       "      <td>137.15</td>\n",
       "      <td>140.15</td>\n",
       "      <td>140.11</td>\n",
       "      <td>221205300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>English</td>\n",
       "      <td>United States</td>\n",
       "      <td>2024-11-18 06:30:00+00:00</td>\n",
       "      <td>2024-11-18</td>\n",
       "      <td>https://247wallst.com/market-news/2024/11/17/n...</td>\n",
       "      <td>Nasdaq Futures Up Sunday Night : NVIDIA Earnin...</td>\n",
       "      <td>247wallst.com</td>\n",
       "      <td>139.5</td>\n",
       "      <td>141.55</td>\n",
       "      <td>137.15</td>\n",
       "      <td>140.15</td>\n",
       "      <td>140.11</td>\n",
       "      <td>221205300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>English</td>\n",
       "      <td>United States</td>\n",
       "      <td>2024-11-18 11:00:00+00:00</td>\n",
       "      <td>2024-11-18</td>\n",
       "      <td>https://www.benzinga.com/24/11/42029943/dow-tu...</td>\n",
       "      <td>Dow Tumbles Over 300 Points Following Economic...</td>\n",
       "      <td>benzinga.com</td>\n",
       "      <td>139.5</td>\n",
       "      <td>141.55</td>\n",
       "      <td>137.15</td>\n",
       "      <td>140.15</td>\n",
       "      <td>140.11</td>\n",
       "      <td>221205300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language  sourcecountry                   seendate       date  \\\n",
       "0  English      Australia  2024-11-18 03:45:00+00:00 2024-11-18   \n",
       "1  English         Cyprus  2024-11-18 04:00:00+00:00 2024-11-18   \n",
       "2  English          China  2024-11-18 04:00:00+00:00 2024-11-18   \n",
       "3  English  United States  2024-11-18 06:30:00+00:00 2024-11-18   \n",
       "4  English  United States  2024-11-18 11:00:00+00:00 2024-11-18   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.fool.com.au/2024/11/18/prediction-...   \n",
       "1  https://cyprus-mail.com/2024/11/18/softbank-fi...   \n",
       "2  https://www.morningstar.com/markets/this-unlov...   \n",
       "3  https://247wallst.com/market-news/2024/11/17/n...   \n",
       "4  https://www.benzinga.com/24/11/42029943/dow-tu...   \n",
       "\n",
       "                                               title           domain   open  \\\n",
       "0  Prediction : Nvidia stock is going to soar aft...      fool.com.au  139.5   \n",
       "1  SoftBank first to receive new Nvidia chips for...  cyprus-mail.com  139.5   \n",
       "2  Why Small - Cap Value Stocks Look Attractive R...  morningstar.com  139.5   \n",
       "3  Nasdaq Futures Up Sunday Night : NVIDIA Earnin...    247wallst.com  139.5   \n",
       "4  Dow Tumbles Over 300 Points Following Economic...     benzinga.com  139.5   \n",
       "\n",
       "     high     low   close  adj_close     volume  \n",
       "0  141.55  137.15  140.15     140.11  221205300  \n",
       "1  141.55  137.15  140.15     140.11  221205300  \n",
       "2  141.55  137.15  140.15     140.11  221205300  \n",
       "3  141.55  137.15  140.15     140.11  221205300  \n",
       "4  141.55  137.15  140.15     140.11  221205300  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load tfidf matrices\n",
    "X_train_text = sparse.load_npz(f\"{DATA_PATH}/tfidf/X_train_tfidf.npz\")\n",
    "X_test_text  = sparse.load_npz(f\"{DATA_PATH}/tfidf/X_test_tfidf.npz\")\n",
    "vectorizer   = joblib.load(f\"{DATA_PATH}/tfidf/tfidf_vectorizer.pkl\")\n",
    "\n",
    "df_nvidia = pd.read_csv(DATA_PATH / \"NVIDIA_Merged_20241101-Present.csv\")\n",
    "df_nvidia['date'] = pd.to_datetime(df_nvidia['date'])\n",
    "df_nvidia.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de253633",
   "metadata": {},
   "source": [
    "# 3. Train/Test Split (Daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a76f475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train days: 226\n",
      "Test days : 15\n",
      "TF-IDF train shape: (226, 50)\n",
      "TF-IDF test shape : (15, 50)\n"
     ]
    }
   ],
   "source": [
    "SPLIT_DATE = pd.Timestamp(\"2025-11-01\")\n",
    "\n",
    "train_df = df_nvidia[df_nvidia[\"date\"] < SPLIT_DATE]\n",
    "test_df  = df_nvidia[df_nvidia[\"date\"] >= SPLIT_DATE]\n",
    "\n",
    "train_daily = (\n",
    "    train_df.groupby(\"date\")[\"close\"]\n",
    "    .first()\n",
    "    .reset_index()\n",
    "    .sort_values(\"date\")\n",
    ")\n",
    "\n",
    "test_daily = (\n",
    "    test_df.groupby(\"date\")[\"close\"]\n",
    "    .first()\n",
    "    .reset_index()\n",
    "    .sort_values(\"date\")\n",
    ")\n",
    "\n",
    "train_dates = train_daily[\"date\"].values\n",
    "test_dates  = test_daily[\"date\"].values\n",
    "\n",
    "y_train_all = train_daily[\"close\"].values\n",
    "y_test_all  = test_daily[\"close\"].values\n",
    "\n",
    "print(\"Train days:\", len(train_dates))\n",
    "print(\"Test days :\", len(test_dates))\n",
    "print(\"TF-IDF train shape:\", X_train_text.shape)\n",
    "print(\"TF-IDF test shape :\", X_test_text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127bb3d8",
   "metadata": {},
   "source": [
    "# 4. Compute Returns & Time-Series Alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "623a9e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_returns = (y_train_all[1:] - y_train_all[:-1]) / y_train_all[:-1]   # length N-1\n",
    "test_returns  = (y_test_all[1:] - y_test_all[:-1]) / y_test_all[:-1]      # length M-1\n",
    "\n",
    "first_test_return_prev = (y_test_all[0] - y_train_all[-1]) / y_train_all[-1]\n",
    "test_returns = np.r_[first_test_return_prev, test_returns]  # length M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c85dc21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF aligned: (224, 50)\n",
      "Returns aligned: (224, 1)\n",
      "Labels aligned : (224,)\n"
     ]
    }
   ],
   "source": [
    "# Labels: movement(t)\n",
    "y_train = (train_returns[1:] > 0).astype(int)            # length N-2\n",
    "\n",
    "# Feature: return(t-1)\n",
    "prev_return_train = train_returns[:-1].reshape(-1, 1)     # length N-2\n",
    "\n",
    "# Feature: TF-IDF(t-1)\n",
    "X_train_tfidf_prev = X_train_text[:-2]                    # length N-2\n",
    "\n",
    "print(\"TF-IDF aligned:\", X_train_tfidf_prev.shape)\n",
    "print(\"Returns aligned:\", prev_return_train.shape)\n",
    "print(\"Labels aligned :\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d172f221",
   "metadata": {},
   "source": [
    "# 5. GridSearch for Optimal Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7f2eddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training SVD=30 ===\n",
      "\n",
      "=== Training SVD=50 ===\n"
     ]
    }
   ],
   "source": [
    "svd_dims = [30, 50]\n",
    "thresholds = np.arange(0.4, 0.61, 0.025)\n",
    "\n",
    "results = []\n",
    "\n",
    "for dim in svd_dims:\n",
    "    print(f\"\\n=== Training SVD={dim} ===\")\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=dim, random_state=42)\n",
    "    X_train_svd = svd.fit_transform(X_train_tfidf_prev)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    prev_return_scaled = scaler.fit_transform(prev_return_train)\n",
    "    \n",
    "    X_train_iter = np.hstack([X_train_svd, prev_return_scaled])\n",
    "    \n",
    "    # hyperparameter search\n",
    "    param_grid = {\n",
    "        \"penalty\": [\"l1\", \"l2\"],\n",
    "        \"C\": [0.01, 0.1, 1, 5, 10],\n",
    "        \"solver\": [\"saga\"],\n",
    "        \"max_iter\": [2000],\n",
    "    }\n",
    "    \n",
    "    logreg = LogisticRegression(class_weight=\"balanced\")\n",
    "    \n",
    "    grid = GridSearchCV(\n",
    "        logreg, param_grid, scoring=\"f1\", cv=5, n_jobs=-1, verbose=0\n",
    "    )\n",
    "    grid.fit(X_train_iter, y_train)\n",
    "    \n",
    "    best_model = grid.best_estimator_\n",
    "    \n",
    "    # prediction\n",
    "    X_test_svd = svd.transform(X_test_text)\n",
    "    \n",
    "    predicted_probs = []\n",
    "    prev_ret = train_returns[-1]\n",
    "    \n",
    "    for i in range(len(test_dates)):\n",
    "        prev_scaled = scaler.transform([[prev_ret]])\n",
    "        X_i = np.hstack([X_test_svd[i].reshape(1, -1), prev_scaled])\n",
    "        \n",
    "        prob_up = best_model.predict_proba(X_i)[0][1]\n",
    "        predicted_probs.append(prob_up)\n",
    "        \n",
    "        prev_ret = test_returns[i]\n",
    "    \n",
    "    actual = (test_returns > 0).astype(int)\n",
    "    \n",
    "    # evaluate thresholds\n",
    "    for thr in thresholds:\n",
    "        preds = (np.array(predicted_probs) >= thr).astype(int)\n",
    "        \n",
    "        acc = accuracy_score(actual, preds)\n",
    "        prec = precision_score(actual, preds, zero_division=0)\n",
    "        rec  = recall_score(actual, preds)\n",
    "        f1   = f1_score(actual, preds)\n",
    "        \n",
    "        results.append({\n",
    "            \"svd_dim\": dim,\n",
    "            \"threshold\": thr,\n",
    "            \"accuracy\": acc,\n",
    "            \"precision\": prec,\n",
    "            \"recall\": rec,\n",
    "            \"f1\": f1,\n",
    "            \"confusion\": confusion_matrix(actual, preds)\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d3226",
   "metadata": {},
   "source": [
    "# 6. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f81efa7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>svd_dim</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>confusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>[[5, 4], [0, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>50</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>[[5, 4], [0, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>[[8, 1], [2, 4]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>50</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>[[8, 1], [2, 4]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>[[9, 0], [3, 3]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>50</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>[[9, 0], [3, 3]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>[[1, 8], [0, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>50</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>[[1, 8], [0, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>[[0, 9], [0, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>50</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>[[0, 9], [0, 6]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    svd_dim  threshold  accuracy  precision    recall        f1  \\\n",
       "2        30      0.450  0.733333   0.600000  1.000000  0.750000   \n",
       "11       50      0.450  0.733333   0.600000  1.000000  0.750000   \n",
       "3        30      0.475  0.800000   0.800000  0.666667  0.727273   \n",
       "12       50      0.475  0.800000   0.800000  0.666667  0.727273   \n",
       "4        30      0.500  0.800000   1.000000  0.500000  0.666667   \n",
       "13       50      0.500  0.800000   1.000000  0.500000  0.666667   \n",
       "1        30      0.425  0.466667   0.428571  1.000000  0.600000   \n",
       "10       50      0.425  0.466667   0.428571  1.000000  0.600000   \n",
       "0        30      0.400  0.400000   0.400000  1.000000  0.571429   \n",
       "9        50      0.400  0.400000   0.400000  1.000000  0.571429   \n",
       "\n",
       "           confusion  \n",
       "2   [[5, 4], [0, 6]]  \n",
       "11  [[5, 4], [0, 6]]  \n",
       "3   [[8, 1], [2, 4]]  \n",
       "12  [[8, 1], [2, 4]]  \n",
       "4   [[9, 0], [3, 3]]  \n",
       "13  [[9, 0], [3, 3]]  \n",
       "1   [[1, 8], [0, 6]]  \n",
       "10  [[1, 8], [0, 6]]  \n",
       "0   [[0, 9], [0, 6]]  \n",
       "9   [[0, 9], [0, 6]]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df_sorted = results_df.sort_values(\"f1\", ascending=False)\n",
    "\n",
    "results_df_sorted.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7920cb7",
   "metadata": {},
   "source": [
    "The grid search identified a clear optimal configuration for predicting NVIDIA's next-day stock direction. The best model used 30 SVD components (or 50, given performance was equivalent) and a classification threshold of 0.45, achieving an f1 score of 0.75. This configuration yielded:\n",
    "- 100% recall, meaning the model successfully identified all upward price movements\n",
    "- 60% precision, indicating some false positives but acceptable given the strong recall\n",
    "- Overall accuracy of 73.33%, performing better than random guessing (50%)\n",
    "\n",
    "Some general patterns from the results:\n",
    "- Threshold tuning plays a larger role in the model performance than SVD dimensionality of the tfidf vectors, given that both 30 and 50 tfidf components preserved identical amounts of semantic information from article headlines (only changes precision-recall balance)\n",
    "- At lower thresholds (0.40-0.425), the model becomes over-aggressive and predicts 'up' on almost every day, which inflates recall and harms precision\n",
    "- At higher thresholds (0.475-0.50), the model becomes too conservative and misses many upward movements\n",
    "- The models tend to perform better at identifying 'up' movements than 'down' movements, which is a realistic pattern within the financial market given market asymmetry and the positive skew of headline sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2848565",
   "metadata": {},
   "source": [
    "# 7. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d3eeac",
   "metadata": {},
   "source": [
    "The results suggest that daily financial headlines contain meaningful predictive signals about NVIDIA's next-day stock direction, especially when combined with the previous day's return. Even a simple linear classifier like logistic regression with L1 regularization can extract enough information from the tfidf values and returns to achieve decent directional accuracy and strong recall.\n",
    "\n",
    "However, the model's predictive strength is asymmetric, given it is highly sensitive to upward momentum (good recall) but is less reliable at identifying downward movements (occasional false positives). This asymmetry aligns with known properties of financial news, namely, headlines often express positive or optimistic sentiment through bias, and downward market moves are harder to anticipate and might be driven by external shocks that are not captured in daily news. Again, given the similarity between the 30-component and 50-component SVD of the tfidf embeddings, this suggests the tfidf embeddings contain relatively low-rank signals, and increasing embedding dimensionality does not meaningfully improve the predictive power of our model.\n",
    "\n",
    "Some potential next steps we could take are:\n",
    "- Adding sentiment scores using VADER or FinBERT alongside tfidf\n",
    "- Incorporate headline volume (# of articles per day)\n",
    "- Multi-day history features, like 3-7 day rolling averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32cb38b",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8838fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep features\n",
    "svd_dim = 30  # 30 since 50 had no change\n",
    "\n",
    "svd = TruncatedSVD(n_components=svd_dim, random_state=42)\n",
    "X_train_svd_alt = svd.fit_transform(X_train_tfidf_prev)\n",
    "\n",
    "# scale previous-day returns\n",
    "scaler_ret_alt = StandardScaler()\n",
    "prev_return_scaled_alt = scaler_ret_alt.fit_transform(prev_return_train)\n",
    "\n",
    "# final train matrix\n",
    "X_train_alt = np.hstack([X_train_svd_alt, prev_return_scaled_alt])\n",
    "\n",
    "# test features\n",
    "X_test_svd_alt = svd.transform(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f70b3e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Best XGBoost params: {'colsample_bytree': 0.7, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "xgb = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    "    scale_pos_weight=(y_train==0).sum() / (y_train==1).sum()  # balancing\n",
    ")\n",
    "\n",
    "xgb_param_grid = {\n",
    "    \"n_estimators\": [100, 200, 400],\n",
    "    \"max_depth\": [2, 3, 4],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"subsample\": [0.7, 1.0],\n",
    "    \"colsample_bytree\": [0.7, 1.0]\n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(\n",
    "    xgb,\n",
    "    xgb_param_grid,\n",
    "    scoring=\"f1\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "xgb_grid.fit(X_train_alt, y_train)\n",
    "\n",
    "best_xgb = xgb_grid.best_estimator_\n",
    "print(\"\\nBest XGBoost params:\", xgb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6bd0142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred\n",
    "xgb_probs = []\n",
    "prev_ret = train_returns[-1]\n",
    "\n",
    "for i in range(len(test_dates)):\n",
    "\n",
    "    prev_scaled = scaler_ret_alt.transform([[prev_ret]])\n",
    "    X_i = np.hstack([X_test_svd_alt[i].reshape(1, -1), prev_scaled])\n",
    "\n",
    "    prob_up = best_xgb.predict_proba(X_i)[0][1]\n",
    "    xgb_probs.append(prob_up)\n",
    "\n",
    "    prev_ret = test_returns[i]\n",
    "\n",
    "actual = (test_returns > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1584538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>confusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>[[6, 3], [4, 2]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>[[7, 2], [5, 1]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>[[7, 2], [5, 1]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>[[7, 2], [5, 1]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[8, 1], [6, 0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[8, 1], [6, 0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[9, 0], [6, 0]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  accuracy  precision    recall        f1         confusion\n",
       "0       0.40  0.533333   0.400000  0.333333  0.363636  [[6, 3], [4, 2]]\n",
       "1       0.45  0.533333   0.333333  0.166667  0.222222  [[7, 2], [5, 1]]\n",
       "2       0.50  0.533333   0.333333  0.166667  0.222222  [[7, 2], [5, 1]]\n",
       "3       0.55  0.533333   0.333333  0.166667  0.222222  [[7, 2], [5, 1]]\n",
       "4       0.60  0.533333   0.000000  0.000000  0.000000  [[8, 1], [6, 0]]\n",
       "5       0.65  0.533333   0.000000  0.000000  0.000000  [[8, 1], [6, 0]]\n",
       "6       0.70  0.600000   0.000000  0.000000  0.000000  [[9, 0], [6, 0]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_thresholds = np.arange(0.4, 0.71, 0.05)\n",
    "\n",
    "xgb_results = []\n",
    "\n",
    "for thr in xgb_thresholds:\n",
    "    preds = (np.array(xgb_probs) >= thr).astype(int)\n",
    "\n",
    "    acc = accuracy_score(actual, preds)\n",
    "    prec = precision_score(actual, preds, zero_division=0)\n",
    "    rec  = recall_score(actual, preds)\n",
    "    f1   = f1_score(actual, preds)\n",
    "\n",
    "    xgb_results.append({\n",
    "        \"threshold\": thr,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "        \"confusion\": confusion_matrix(actual, preds)\n",
    "    })\n",
    "\n",
    "xgb_results_df = pd.DataFrame(xgb_results)\n",
    "xgb_results_df.sort_values(\"f1\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2187c1b",
   "metadata": {},
   "source": [
    "# SVM w/ Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4d3d0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "Best SVM params: {'svm__C': 0.01, 'svm__loss': 'squared_hinge', 'svm__max_iter': 2000}\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm\", LinearSVC(class_weight=\"balanced\", random_state=42))\n",
    "])\n",
    "\n",
    "svm_param_grid = {\n",
    "    \"svm__C\": [0.01, 0.1, 1, 5, 10],\n",
    "    \"svm__loss\": [\"squared_hinge\"],\n",
    "    \"svm__max_iter\": [2000, 5000]\n",
    "}\n",
    "\n",
    "svm_grid = GridSearchCV(\n",
    "    svm_clf,\n",
    "    svm_param_grid,\n",
    "    scoring=\"f1\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "svm_grid.fit(X_train_alt, y_train)\n",
    "\n",
    "best_svm = svm_grid.best_estimator_\n",
    "print(\"\\nBest SVM params:\", svm_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34a9f605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred\n",
    "predicted_scores = []\n",
    "prev_ret = train_returns[-1]\n",
    "\n",
    "for i in range(len(test_dates)):\n",
    "\n",
    "    X_i = np.hstack([\n",
    "        X_test_svd_alt[i].reshape(1, -1),\n",
    "        [[prev_ret]]            # raw previous return (unscaled)\n",
    "    ])\n",
    "\n",
    "    margin = best_svm.decision_function(X_i)[0]   # raw SVM margin\n",
    "    predicted_scores.append(margin)\n",
    "\n",
    "    prev_ret = test_returns[i]\n",
    "\n",
    "actual = (test_returns > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "640f7dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>confusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>[[2, 7], [0, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8.000000e-01</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>[[1, 8], [0, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-7.000000e-01</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>[[1, 8], [0, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-6.000000e-01</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>[[1, 8], [0, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-2.000000e-01</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>[[8, 1], [3, 3]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>[[0, 9], [0, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9.000000e-01</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>[[0, 9], [0, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-3.000000e-01</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>[[7, 2], [3, 3]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-4.000000e-01</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[[3, 6], [2, 4]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.000000e-01</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[[9, 0], [4, 2]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-2.220446e-16</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>[[9, 0], [5, 1]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       threshold  accuracy  precision    recall        f1         confusion\n",
       "5  -5.000000e-01  0.533333   0.461538  1.000000  0.631579  [[2, 7], [0, 6]]\n",
       "2  -8.000000e-01  0.466667   0.428571  1.000000  0.600000  [[1, 8], [0, 6]]\n",
       "3  -7.000000e-01  0.466667   0.428571  1.000000  0.600000  [[1, 8], [0, 6]]\n",
       "4  -6.000000e-01  0.466667   0.428571  1.000000  0.600000  [[1, 8], [0, 6]]\n",
       "8  -2.000000e-01  0.733333   0.750000  0.500000  0.600000  [[8, 1], [3, 3]]\n",
       "0  -1.000000e+00  0.400000   0.400000  1.000000  0.571429  [[0, 9], [0, 6]]\n",
       "1  -9.000000e-01  0.400000   0.400000  1.000000  0.571429  [[0, 9], [0, 6]]\n",
       "7  -3.000000e-01  0.666667   0.600000  0.500000  0.545455  [[7, 2], [3, 3]]\n",
       "6  -4.000000e-01  0.466667   0.400000  0.666667  0.500000  [[3, 6], [2, 4]]\n",
       "9  -1.000000e-01  0.733333   1.000000  0.333333  0.500000  [[9, 0], [4, 2]]\n",
       "10 -2.220446e-16  0.666667   1.000000  0.166667  0.285714  [[9, 0], [5, 1]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# threshold sweep\n",
    "thresholds = np.arange(-1.0, 0.1, 0.1)\n",
    "\n",
    "svm_results = []\n",
    "\n",
    "for thr in thresholds:\n",
    "    preds = (np.array(predicted_scores) >= thr).astype(int)\n",
    "\n",
    "    acc = accuracy_score(actual, preds)\n",
    "    prec = precision_score(actual, preds, zero_division=0)\n",
    "    rec  = recall_score(actual, preds)\n",
    "    f1   = f1_score(actual, preds)\n",
    "\n",
    "    svm_results.append({\n",
    "        \"threshold\": thr,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "        \"confusion\": confusion_matrix(actual, preds)\n",
    "    })\n",
    "\n",
    "svm_results_df = pd.DataFrame(svm_results)\n",
    "svm_results_df.sort_values(\"f1\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18a3a41",
   "metadata": {},
   "source": [
    "Overall, across all models evaluated, the core finding is that daily financial news headlines do contain meaningful predictive information about NVIDIA’s next-day stock direction, especially when combined with the previous day's return. Even with a relatively small dataset and simple linear methods, the models were able to extract signals from TF-IDF representations of headlines and perform notably better than random guessing.\n",
    "\n",
    "Logistic regression was the strongest model overall. With a threshold of 0.45, it achieved an F1 score of 0.75, correctly identifying 100% of upward movements with 60% precision and 73.33% accuracy. The model showed that:\n",
    "- Thresholds have a larger impact on performance than SVD dimensionality, suggesting that most meaningful headline information lies in a low-rank structure.\n",
    "- Lower thresholds inflate recall while harming precision, whereas higher thresholds reverse this tradeoff.\n",
    "- The best performance occurs when recall and precision are balanced through threshold tuning.\n",
    "\n",
    "Although XGBoost is a powerful nonlinear model in larger datasets, it performed the worst here. This is expected in small, low-signal datasets like ours, where TF-IDF vectors reduce to limited low-rank embeddings. XGBoost underfit significantly, producing low recall and precision, and struggled to extract meaningful nonlinear structure from the available features.\n",
    "\n",
    "The linear SVM performed okay (better than random guessing). After sweeping decision boundaries in margin space, the best configuration achieved an F1 score of 0.631, with 0.533 accuracy, 1.0 recall, and 0.461 precision. Like logistic regression, SVM performance was highly sensitive to threshold selection, reinforcing that for linear models on this dataset, decision boundary placement is more influential than embedding dimensionality.\n",
    "\n",
    "Across all models, one consistent pattern emerged: predicting “up” is easier than predicting “down.” This asymmetry is realistic and expected, given that financial headlines tend to be positively skewed, markets drift upward over time, and negative movements are often triggered by external shocks not reflected in daily news text. Consequently, the models achieved strong recall for upward movements but were less reliable at identifying downward days. The similar performance between 30-component and 50-component SVD further indicates that the TF-IDF embeddings contain low-rank latent structure, and increasing representation dimensionality does not substantially improve predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80430d5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "buan5312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
