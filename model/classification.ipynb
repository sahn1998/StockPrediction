{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd556906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy import sparse\n",
    "import joblib\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9123fd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shapes:\n",
      "Train: (226, 50)\n",
      "Test : (15, 50)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>sourcecountry</th>\n",
       "      <th>seendate</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>domain</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>Australia</td>\n",
       "      <td>2024-11-18 03:45:00+00:00</td>\n",
       "      <td>2024-11-18</td>\n",
       "      <td>https://www.fool.com.au/2024/11/18/prediction-...</td>\n",
       "      <td>Prediction : Nvidia stock is going to soar aft...</td>\n",
       "      <td>fool.com.au</td>\n",
       "      <td>139.5</td>\n",
       "      <td>141.55</td>\n",
       "      <td>137.15</td>\n",
       "      <td>140.15</td>\n",
       "      <td>140.11</td>\n",
       "      <td>221205300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>English</td>\n",
       "      <td>Cyprus</td>\n",
       "      <td>2024-11-18 04:00:00+00:00</td>\n",
       "      <td>2024-11-18</td>\n",
       "      <td>https://cyprus-mail.com/2024/11/18/softbank-fi...</td>\n",
       "      <td>SoftBank first to receive new Nvidia chips for...</td>\n",
       "      <td>cyprus-mail.com</td>\n",
       "      <td>139.5</td>\n",
       "      <td>141.55</td>\n",
       "      <td>137.15</td>\n",
       "      <td>140.15</td>\n",
       "      <td>140.11</td>\n",
       "      <td>221205300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>China</td>\n",
       "      <td>2024-11-18 04:00:00+00:00</td>\n",
       "      <td>2024-11-18</td>\n",
       "      <td>https://www.morningstar.com/markets/this-unlov...</td>\n",
       "      <td>Why Small - Cap Value Stocks Look Attractive R...</td>\n",
       "      <td>morningstar.com</td>\n",
       "      <td>139.5</td>\n",
       "      <td>141.55</td>\n",
       "      <td>137.15</td>\n",
       "      <td>140.15</td>\n",
       "      <td>140.11</td>\n",
       "      <td>221205300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>English</td>\n",
       "      <td>United States</td>\n",
       "      <td>2024-11-18 06:30:00+00:00</td>\n",
       "      <td>2024-11-18</td>\n",
       "      <td>https://247wallst.com/market-news/2024/11/17/n...</td>\n",
       "      <td>Nasdaq Futures Up Sunday Night : NVIDIA Earnin...</td>\n",
       "      <td>247wallst.com</td>\n",
       "      <td>139.5</td>\n",
       "      <td>141.55</td>\n",
       "      <td>137.15</td>\n",
       "      <td>140.15</td>\n",
       "      <td>140.11</td>\n",
       "      <td>221205300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>English</td>\n",
       "      <td>United States</td>\n",
       "      <td>2024-11-18 11:00:00+00:00</td>\n",
       "      <td>2024-11-18</td>\n",
       "      <td>https://www.benzinga.com/24/11/42029943/dow-tu...</td>\n",
       "      <td>Dow Tumbles Over 300 Points Following Economic...</td>\n",
       "      <td>benzinga.com</td>\n",
       "      <td>139.5</td>\n",
       "      <td>141.55</td>\n",
       "      <td>137.15</td>\n",
       "      <td>140.15</td>\n",
       "      <td>140.11</td>\n",
       "      <td>221205300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language  sourcecountry                   seendate       date  \\\n",
       "0  English      Australia  2024-11-18 03:45:00+00:00 2024-11-18   \n",
       "1  English         Cyprus  2024-11-18 04:00:00+00:00 2024-11-18   \n",
       "2  English          China  2024-11-18 04:00:00+00:00 2024-11-18   \n",
       "3  English  United States  2024-11-18 06:30:00+00:00 2024-11-18   \n",
       "4  English  United States  2024-11-18 11:00:00+00:00 2024-11-18   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.fool.com.au/2024/11/18/prediction-...   \n",
       "1  https://cyprus-mail.com/2024/11/18/softbank-fi...   \n",
       "2  https://www.morningstar.com/markets/this-unlov...   \n",
       "3  https://247wallst.com/market-news/2024/11/17/n...   \n",
       "4  https://www.benzinga.com/24/11/42029943/dow-tu...   \n",
       "\n",
       "                                               title           domain   open  \\\n",
       "0  Prediction : Nvidia stock is going to soar aft...      fool.com.au  139.5   \n",
       "1  SoftBank first to receive new Nvidia chips for...  cyprus-mail.com  139.5   \n",
       "2  Why Small - Cap Value Stocks Look Attractive R...  morningstar.com  139.5   \n",
       "3  Nasdaq Futures Up Sunday Night : NVIDIA Earnin...    247wallst.com  139.5   \n",
       "4  Dow Tumbles Over 300 Points Following Economic...     benzinga.com  139.5   \n",
       "\n",
       "     high     low   close  adj_close     volume  \n",
       "0  141.55  137.15  140.15     140.11  221205300  \n",
       "1  141.55  137.15  140.15     140.11  221205300  \n",
       "2  141.55  137.15  140.15     140.11  221205300  \n",
       "3  141.55  137.15  140.15     140.11  221205300  \n",
       "4  141.55  137.15  140.15     140.11  221205300  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data tfidf matrices, mapping vectorizer, merged dataset\n",
    "DATA_PATH = Path(\"../data/\")\n",
    "\n",
    "X_train_text = sparse.load_npz(DATA_PATH / \"tfidf/X_train_tfidf.npz\")\n",
    "X_test_text  = sparse.load_npz(DATA_PATH / \"tfidf/X_test_tfidf.npz\")\n",
    "vectorizer   = joblib.load(DATA_PATH / \"tfidf/tfidf_vectorizer.pkl\")\n",
    "\n",
    "df_nvidia = pd.read_csv(DATA_PATH / \"NVIDIA_Merged_20241101-Present.csv\")\n",
    "df_nvidia[\"date\"] = pd.to_datetime(df_nvidia[\"date\"])\n",
    "\n",
    "print(\"TF-IDF shapes:\")\n",
    "print(\"Train:\", X_train_text.shape)\n",
    "print(\"Test :\", X_test_text.shape)\n",
    "df_nvidia.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4574d8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train days: 226\n",
      "Test days : 15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(        date   close\n",
       " 0 2024-11-18  140.15\n",
       " 1 2024-11-19  147.01\n",
       " 2 2024-11-20  145.89\n",
       " 3 2024-11-21  146.67\n",
       " 4 2024-11-22  141.95,\n",
       "         date   close\n",
       " 0 2025-11-03  206.88\n",
       " 1 2025-11-04  198.69\n",
       " 2 2025-11-05  195.21\n",
       " 3 2025-11-06  188.08\n",
       " 4 2025-11-07  188.15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train/test split\n",
    "SPLIT_DATE = pd.Timestamp(\"2025-11-01\")\n",
    "\n",
    "train_df = df_nvidia[df_nvidia[\"date\"] < SPLIT_DATE]\n",
    "test_df  = df_nvidia[df_nvidia[\"date\"] >= SPLIT_DATE]\n",
    "\n",
    "# collapse multiple headlines per day & one closing price per day.\n",
    "train_daily = train_df.groupby(\"date\")[\"close\"].first().reset_index().sort_values(\"date\")\n",
    "test_daily  = test_df.groupby(\"date\")[\"close\"].first().reset_index().sort_values(\"date\")\n",
    "\n",
    "y_train_all = train_daily[\"close\"].values\n",
    "y_test_all  = test_daily[\"close\"].values\n",
    "\n",
    "print(f\"Train days: {len(train_daily)}\")\n",
    "print(f\"Test days : {len(test_daily)}\")\n",
    "train_daily.head(), test_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fa51735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train returns length: 225\n",
      "Test returns length : 15\n",
      "Sample train returns: [ 0.04894756 -0.00761853  0.00534649 -0.03218109 -0.04177527]\n",
      "Sample test returns : [ 0.02168008 -0.03958817 -0.01751472 -0.03652477  0.00037218]\n"
     ]
    }
   ],
   "source": [
    "# daily returns\n",
    "# r(t) = (close[t] − close[t−1]) / close[t−1]\n",
    "train_returns = (y_train_all[1:] - y_train_all[:-1]) / y_train_all[:-1]\n",
    "test_returns  = (y_test_all[1:]  - y_test_all[:-1])  / y_test_all[:-1]\n",
    "\n",
    "# handle last-train and first-test boundary for walk-forward\n",
    "first_test_return_prev = (y_test_all[0] - y_train_all[-1]) / y_train_all[-1]\n",
    "test_returns = np.r_[first_test_return_prev, test_returns]\n",
    "\n",
    "print(\"Train returns length:\", len(train_returns))\n",
    "print(\"Test returns length :\", len(test_returns))\n",
    "print(\"Sample train returns:\", train_returns[:5])\n",
    "print(\"Sample test returns :\", test_returns[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0144afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window 1 shapes:\n",
      "TF-IDF: (224, 50)\n",
      "Returns: (224, 1)\n",
      "Labels: (224,)\n"
     ]
    }
   ],
   "source": [
    "# window size 1 alignment (baseline \"yesterday predicts tomorrow\")\n",
    "# feature: r(t-1)\n",
    "# label: movement(t)\n",
    "y_train_w1 = (train_returns[1:] > 0).astype(int)\n",
    "prev_return_train_w1 = train_returns[:-1].reshape(-1, 1)\n",
    "\n",
    "# align tfidf by trimming last 2 days.\n",
    "X_train_tfidf_prev_w1 = X_train_text[:-2]\n",
    "\n",
    "print(\"Window 1 shapes:\")\n",
    "print(\"TF-IDF:\", X_train_tfidf_prev_w1.shape)\n",
    "print(\"Returns:\", prev_return_train_w1.shape)\n",
    "print(\"Labels:\", y_train_w1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc4bf5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window 3 shapes:\n",
      "TF-IDF: (222, 50)\n",
      "Returns: (222, 3)\n",
      "Labels: (222,)\n"
     ]
    }
   ],
   "source": [
    "# window size 3 alignment (short-term autocorrelation)\n",
    "# features: [r(t-1), r(t-2), r(t-3)]\n",
    "# label: movement(t)\n",
    "def make_forward_window(arr, window):\n",
    "    L = len(arr)\n",
    "    feats = np.column_stack([arr[i : i + (L - window)] for i in range(window)])\n",
    "    labels = arr[window:]\n",
    "    return feats, labels\n",
    "\n",
    "prev_return_train_w3, y_raw_w3 = make_forward_window(train_returns, 3)\n",
    "y_train_w3 = (y_raw_w3 > 0).astype(int)\n",
    "\n",
    "# drop last 4 days to align tfidf\n",
    "X_train_tfidf_prev_w3 = X_train_text[:-4]\n",
    "\n",
    "print(\"Window 3 shapes:\")\n",
    "print(\"TF-IDF:\", X_train_tfidf_prev_w3.shape)\n",
    "print(\"Returns:\", prev_return_train_w3.shape)\n",
    "print(\"Labels:\", y_train_w3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c96c7289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD output example (first row):\n",
      "[ 0.84020023  0.13477277 -0.20840343 -0.07820849 -0.10059298  0.17792114\n",
      "  0.10120211 -0.0534392  -0.11416216 -0.20409693]\n",
      "\n",
      "PCA output example (first row):\n",
      "[ 0.12654047 -0.21365639 -0.08439016 -0.0955109   0.16770294 -0.1143939\n",
      "  0.08081019  0.03170237 -0.21525048  0.19806781]\n"
     ]
    }
   ],
   "source": [
    "# svd & pca\n",
    "svd_dim = 50\n",
    "svd = TruncatedSVD(n_components=svd_dim, random_state=5312)\n",
    "\n",
    "# fit on window-1 tfidf (most data)\n",
    "X_train_svd_w1 = svd.fit_transform(X_train_tfidf_prev_w1)\n",
    "X_train_svd_w3 = svd.transform(X_train_tfidf_prev_w3)\n",
    "X_test_svd     = svd.transform(X_test_text)\n",
    "\n",
    "print(\"SVD output example (first row):\")\n",
    "print(X_train_svd_w1[0][:10])\n",
    "\n",
    "# pca only for logistic regression\n",
    "pca_dim = 10\n",
    "pca = PCA(n_components=pca_dim, random_state=5312)\n",
    "\n",
    "X_train_pca_w1 = pca.fit_transform(X_train_svd_w1)\n",
    "X_train_pca_w3 = pca.transform(X_train_svd_w3)\n",
    "X_test_pca     = pca.transform(X_test_svd)\n",
    "\n",
    "print(\"\\nPCA output example (first row):\")\n",
    "print(X_train_pca_w1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72124e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled return example (w1): [[ 1.42049155]\n",
      " [-0.29819081]\n",
      " [ 0.09573351]]\n",
      "Scaled return example (w3): [[ 1.41857385 -0.29515325  0.10081444]\n",
      " [-0.29688575  0.09919488 -1.03969798]\n",
      " [ 0.09629991 -1.04225554 -1.33127789]]\n"
     ]
    }
   ],
   "source": [
    "# scale returns for lr/xgb/pca models for better results\n",
    "# svm uses its own internal scaling\n",
    "scaler_w1 = StandardScaler()\n",
    "prev_return_scaled_w1 = scaler_w1.fit_transform(prev_return_train_w1)\n",
    "\n",
    "scaler_w3 = StandardScaler()\n",
    "prev_return_scaled_w3 = scaler_w3.fit_transform(prev_return_train_w3)\n",
    "\n",
    "print(\"Scaled return example (w1):\", prev_return_scaled_w1[:3])\n",
    "print(\"Scaled return example (w3):\", prev_return_scaled_w3[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60076188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature block shapes:\n",
      "lr_xgb_w1 (224, 51)\n",
      "svm_w1 (224, 51)\n",
      "pca_w1 (224, 11)\n",
      "lr_xgb_w3 (222, 53)\n",
      "svm_w3 (222, 53)\n",
      "pca_w3 (222, 13)\n"
     ]
    }
   ],
   "source": [
    "# training matrices\n",
    "feature_blocks = {\n",
    "    # window 1 models\n",
    "    \"lr_xgb_w1\": np.hstack([X_train_svd_w1, prev_return_scaled_w1]),\n",
    "    \"svm_w1\":    np.hstack([X_train_svd_w1, prev_return_train_w1]),\n",
    "    \"pca_w1\":    np.hstack([X_train_pca_w1, prev_return_scaled_w1]),\n",
    "\n",
    "    # window 3 models\n",
    "    \"lr_xgb_w3\": np.hstack([X_train_svd_w3, prev_return_scaled_w3]),\n",
    "    \"svm_w3\":    np.hstack([X_train_svd_w3, prev_return_train_w3]),\n",
    "    \"pca_w3\":    np.hstack([X_train_pca_w3, prev_return_scaled_w3]),\n",
    "}\n",
    "\n",
    "print(\"\\nFeature block shapes:\")\n",
    "for name, mat in feature_blocks.items():\n",
    "    print(name, mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19aca10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# walk-forward prediction function\n",
    "# simulates real-world forecasting since:\n",
    "# - we predict day t using only info available up to day t-1\n",
    "# - after predicting, we update the return window using the actual return\n",
    "# - prevents lookahead bias\n",
    "def walk_forward_predict(model, X_test_embed, test_returns, last_train_returns,scaler_prev=None, use_proba=True, window=1):\n",
    "    scores = []\n",
    "\n",
    "    # initialize rolling state\n",
    "    if window == 1:\n",
    "        state = float(last_train_returns)\n",
    "    else:\n",
    "        state = np.array(last_train_returns).astype(float)  # shape (3,)\n",
    "\n",
    "    print(\"Initial return state:\", state)\n",
    "\n",
    "    for i in range(len(test_returns)):\n",
    "        # build return feature\n",
    "        if window == 1:\n",
    "            ret_feat = np.array([[state]])\n",
    "        else:\n",
    "            ret_feat = state.reshape(1, -1)\n",
    "\n",
    "        # scale returns if scaler provided\n",
    "        if scaler_prev is not None:\n",
    "            ret_feat = scaler_prev.transform(ret_feat)\n",
    "\n",
    "        # build complete feature vector\n",
    "        X_i = np.hstack([X_test_embed[i].reshape(1, -1), ret_feat])\n",
    "\n",
    "        # predict using appropriate 'score'\n",
    "        if use_proba:\n",
    "            prob_up = model.predict_proba(X_i)[0][1]\n",
    "            scores.append(prob_up)\n",
    "        else:\n",
    "            margin = model.decision_function(X_i)[0]\n",
    "            scores.append(margin)\n",
    "\n",
    "        # update return window using actual realized returns\n",
    "        if window == 1:\n",
    "            state = test_returns[i]\n",
    "        else:\n",
    "            state = np.roll(state, -1)\n",
    "            state[-1] = test_returns[i]\n",
    "\n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0c1a65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval thresholds for each model\n",
    "# models produce scores:\n",
    "# - logistic regression / xgb produce probabilities\n",
    "# - svm produce decision-function margins\n",
    "# convert scores into 0/1 labels via thresholds\n",
    "# sweeping thresholds reveals the best accuracy/precision/recall/F1 tradeoff\n",
    "def evaluate_thresholds(raw_scores, actual, thresholds, model_name, svd_dim, window):\n",
    "    rows = []\n",
    "\n",
    "    for thr in thresholds:\n",
    "        preds = (raw_scores >= thr).astype(int)\n",
    "\n",
    "        rows.append({\n",
    "            \"model\": model_name,\n",
    "            \"svd_dim\": svd_dim,\n",
    "            \"window\": window,\n",
    "            \"threshold\": thr,\n",
    "            \"accuracy\": accuracy_score(actual, preds),\n",
    "            \"precision\": precision_score(actual, preds, zero_division=0),\n",
    "            \"recall\": recall_score(actual, preds),\n",
    "            \"f1\": f1_score(actual, preds),\n",
    "            \"confusion\": confusion_matrix(actual, preds)\n",
    "        })\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "352b9014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "# each model is defined with:\n",
    "# - initialization\n",
    "# - hyperparameter grid\n",
    "# - whether it uses probabilities\n",
    "# - which feature matrix to use\n",
    "# - window size (1 or 3)\n",
    "# - whether to scale the return window\n",
    "models = {\n",
    "    # logistic regression (svd)\n",
    "    \"logreg_w1\": {\n",
    "        \"init\": LogisticRegression(class_weight=\"balanced\", solver=\"lbfgs\"),\n",
    "        \"param_grid\": {\"C\": [0.1, 0.5, 1, 5, 10], \"penalty\": [\"l2\"], \"max_iter\": [2000]},\n",
    "        \"use_proba\": True,\n",
    "        \"thresholds\": np.arange(0.40, 0.61, 0.025),\n",
    "        \"train_matrix\": \"lr_xgb_w1\",\n",
    "        \"window\": 1,\n",
    "        \"scale_returns\": True,\n",
    "    },\n",
    "    \"logreg_w3\": {\n",
    "        \"init\": LogisticRegression(class_weight=\"balanced\", solver=\"lbfgs\"),\n",
    "        \"param_grid\": {\"C\": [0.1, 0.5, 1, 5, 10], \"penalty\": [\"l2\"], \"max_iter\": [2000]},\n",
    "        \"use_proba\": True,\n",
    "        \"thresholds\": np.arange(0.40, 0.61, 0.025),\n",
    "        \"train_matrix\": \"lr_xgb_w3\",\n",
    "        \"window\": 3,\n",
    "        \"scale_returns\": True,\n",
    "    },\n",
    "\n",
    "    # xgboost\n",
    "    \"xgboost_w1\": {\n",
    "        \"init\": XGBClassifier(objective=\"binary:logistic\", eval_metric=\"logloss\", random_state=5312),\n",
    "        \"param_grid\": {\n",
    "            \"n_estimators\": [100, 200],\n",
    "            \"max_depth\": [2, 3],\n",
    "            \"learning_rate\": [0.01, 0.05, 0.1, 0.15],\n",
    "        },\n",
    "        \"use_proba\": True,\n",
    "        \"thresholds\": np.arange(0.40, 0.71, 0.05),\n",
    "        \"train_matrix\": \"lr_xgb_w1\",\n",
    "        \"window\": 1,\n",
    "        \"scale_returns\": True,\n",
    "    },\n",
    "    \"xgboost_w3\": {\n",
    "        \"init\": XGBClassifier(objective=\"binary:logistic\", eval_metric=\"logloss\", random_state=5312),\n",
    "        \"param_grid\": {\n",
    "            \"n_estimators\": [100, 200],\n",
    "            \"max_depth\": [2, 3],\n",
    "            \"learning_rate\": [0.01, 0.05, 0.1, 0.15],\n",
    "        },\n",
    "        \"use_proba\": True,\n",
    "        \"thresholds\": np.arange(0.40, 0.71, 0.05),\n",
    "        \"train_matrix\": \"lr_xgb_w3\",\n",
    "        \"window\": 3,\n",
    "        \"scale_returns\": True,\n",
    "    },\n",
    "\n",
    "    # linear svm\n",
    "    \"svm_w1\": {\n",
    "        \"init\": Pipeline([(\"scaler\", StandardScaler()), \n",
    "                          (\"svm\", LinearSVC(class_weight=\"balanced\", random_state=5312))]),\n",
    "        \"param_grid\": {\"svm__C\": [0.01, 0.05, 0.1, 1, 5], \"svm__loss\": [\"squared_hinge\"], \"svm__max_iter\": [2000]},\n",
    "        \"use_proba\": False,\n",
    "        \"thresholds\": np.arange(-1.0, 0.1, 0.1),\n",
    "        \"train_matrix\": \"svm_w1\",\n",
    "        \"window\": 1,\n",
    "        \"scale_returns\": False,\n",
    "    },\n",
    "    \"svm_w3\": {\n",
    "        \"init\": Pipeline([(\"scaler\", StandardScaler()), \n",
    "                          (\"svm\", LinearSVC(class_weight=\"balanced\", random_state=5312))]),\n",
    "        \"param_grid\": {\"svm__C\": [0.01, 0.05, 0.1, 1, 5], \"svm__loss\": [\"squared_hinge\"], \"svm__max_iter\": [2000]},\n",
    "        \"use_proba\": False,\n",
    "        \"thresholds\": np.arange(-1.0, 0.1, 0.1),\n",
    "        \"train_matrix\": \"svm_w3\",\n",
    "        \"window\": 3,\n",
    "        \"scale_returns\": False,\n",
    "    },\n",
    "\n",
    "    # logistic regression (pca)\n",
    "    \"logreg_pca_w1\": {\n",
    "        \"init\": LogisticRegression(class_weight=\"balanced\", solver=\"lbfgs\"),\n",
    "        \"param_grid\": {\"C\": [0.1, 0.5, 1, 5, 10], \"penalty\": [\"l2\"], \"max_iter\": [2000]},\n",
    "        \"use_proba\": True,\n",
    "        \"thresholds\": np.arange(0.40, 0.61, 0.025),\n",
    "        \"train_matrix\": \"pca_w1\",\n",
    "        \"window\": 1,\n",
    "        \"scale_returns\": True,\n",
    "    },\n",
    "    \"logreg_pca_w3\": {\n",
    "        \"init\": LogisticRegression(class_weight=\"balanced\", solver=\"lbfgs\"),\n",
    "        \"param_grid\": {\"C\": [0.1, 0.5, 1, 5, 10], \"penalty\": [\"l2\"], \"max_iter\": [2000]},\n",
    "        \"use_proba\": True,\n",
    "        \"thresholds\": np.arange(0.40, 0.61, 0.025),\n",
    "        \"train_matrix\": \"pca_w3\",\n",
    "        \"window\": 3,\n",
    "        \"scale_returns\": True,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73a6d882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Training LOGREG_W1 ==========\n",
      "Train matrix shape: (224, 51)\n",
      "Label shape: (224,)\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best parameters: {'C': 0.5, 'max_iter': 2000, 'penalty': 'l2'}\n",
      "Initial return state: -0.0019715116565625574\n",
      "Sample prediction scores: [0.45594738 0.42206082 0.49578346 0.46101798 0.51137637]\n",
      "\n",
      "========== Training LOGREG_W3 ==========\n",
      "Train matrix shape: (222, 53)\n",
      "Label shape: (222,)\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best parameters: {'C': 0.1, 'max_iter': 2000, 'penalty': 'l2'}\n",
      "Initial return state: [ 0.02989604 -0.02004444 -0.00197151]\n",
      "Sample prediction scores: [0.41766518 0.53955615 0.55663472 0.40850523 0.58031411]\n",
      "\n",
      "========== Training XGBOOST_W1 ==========\n",
      "Train matrix shape: (224, 51)\n",
      "Label shape: (224,)\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Best parameters: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100}\n",
      "Initial return state: -0.0019715116565625574\n",
      "Sample prediction scores: [0.42610776 0.6413468  0.59130985 0.47720867 0.6418469 ]\n",
      "\n",
      "========== Training XGBOOST_W3 ==========\n",
      "Train matrix shape: (222, 53)\n",
      "Label shape: (222,)\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Best parameters: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 200}\n",
      "Initial return state: [ 0.02989604 -0.02004444 -0.00197151]\n",
      "Sample prediction scores: [0.4616973  0.87238306 0.7792298  0.13294263 0.5050884 ]\n",
      "\n",
      "========== Training SVM_W1 ==========\n",
      "Train matrix shape: (224, 51)\n",
      "Label shape: (224,)\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best parameters: {'svm__C': 1, 'svm__loss': 'squared_hinge', 'svm__max_iter': 2000}\n",
      "Initial return state: -0.0019715116565625574\n",
      "Sample prediction scores: [-0.12032466 -0.80301548 -0.34978019 -0.26257704  0.28104765]\n",
      "\n",
      "========== Training SVM_W3 ==========\n",
      "Train matrix shape: (222, 53)\n",
      "Label shape: (222,)\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best parameters: {'svm__C': 0.01, 'svm__loss': 'squared_hinge', 'svm__max_iter': 2000}\n",
      "Initial return state: [ 0.02989604 -0.02004444 -0.00197151]\n",
      "Sample prediction scores: [ 0.43851593 -0.21384107  0.43941765 -0.33127148  0.29421655]\n",
      "\n",
      "========== Training LOGREG_PCA_W1 ==========\n",
      "Train matrix shape: (224, 11)\n",
      "Label shape: (224,)\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best parameters: {'C': 1, 'max_iter': 2000, 'penalty': 'l2'}\n",
      "Initial return state: -0.0019715116565625574\n",
      "Sample prediction scores: [0.42503735 0.42292439 0.44116826 0.43192548 0.482737  ]\n",
      "\n",
      "========== Training LOGREG_PCA_W3 ==========\n",
      "Train matrix shape: (222, 13)\n",
      "Label shape: (222,)\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best parameters: {'C': 0.1, 'max_iter': 2000, 'penalty': 'l2'}\n",
      "Initial return state: [ 0.02989604 -0.02004444 -0.00197151]\n",
      "Sample prediction scores: [0.41339629 0.53909071 0.54731393 0.40982745 0.58155198]\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "master_results = []\n",
    "\n",
    "for name, cfg in models.items():\n",
    "    print(f\"\\n========== Training {name.upper()} ==========\")\n",
    "\n",
    "    tm = cfg[\"train_matrix\"]\n",
    "    window = cfg[\"window\"]\n",
    "\n",
    "    # select train matrix\n",
    "    X_train_model = feature_blocks[tm]\n",
    "\n",
    "    # select labels based on window size\n",
    "    y_train_model = y_train_w1 if window == 1 else y_train_w3\n",
    "\n",
    "    print(\"Train matrix shape:\", X_train_model.shape)\n",
    "    print(\"Label shape:\", y_train_model.shape)\n",
    "\n",
    "    # determine SVD vs PCA embedding for test set\n",
    "    X_test_embed = X_test_pca if tm.startswith(\"pca_\") else X_test_svd\n",
    "\n",
    "    # gridsearch\n",
    "    grid = GridSearchCV(\n",
    "        cfg[\"init\"], \n",
    "        cfg[\"param_grid\"], \n",
    "        scoring=\"f1\",\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    grid.fit(X_train_model, y_train_model)\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "    print(\"Best parameters:\", grid.best_params_)\n",
    "\n",
    "    # select return scaling\n",
    "    scaler_prev = None\n",
    "    if cfg[\"scale_returns\"]:\n",
    "        scaler_prev = scaler_w1 if window == 1 else scaler_w3\n",
    "\n",
    "    # select initial return window\n",
    "    last_train_returns = train_returns[-window:] if window == 3 else train_returns[-1]\n",
    "\n",
    "    # walk-forward prediction\n",
    "    raw_scores = walk_forward_predict(\n",
    "        best_model,\n",
    "        X_test_embed,\n",
    "        test_returns,\n",
    "        last_train_returns,\n",
    "        scaler_prev=scaler_prev,\n",
    "        use_proba=cfg[\"use_proba\"],\n",
    "        window=window\n",
    "    )\n",
    "\n",
    "    print(\"Sample prediction scores:\", raw_scores[:5])\n",
    "\n",
    "    # convert test returns to binary labels\n",
    "    actual = (test_returns > 0).astype(int)\n",
    "\n",
    "    # evaluate thresholds\n",
    "    rows = evaluate_thresholds(\n",
    "        raw_scores,\n",
    "        actual,\n",
    "        cfg[\"thresholds\"],\n",
    "        name,\n",
    "        svd_dim=50,\n",
    "        window=window\n",
    "    )\n",
    "    master_results.extend(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9486425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 15 model threshold combinations:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>svd_dim</th>\n",
       "      <th>window</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>confusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logreg_w1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>[[6, 3], [0, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>logreg_pca_w1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>[[9, 0], [2, 4]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>logreg_pca_w1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>[[7, 2], [1, 5]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logreg_w1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>[[8, 1], [2, 4]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>svm_w1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>[[6, 3], [1, 5]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>logreg_pca_w1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>[[9, 0], [3, 3]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>svm_w1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>[[5, 4], [1, 5]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logreg_w1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>[[3, 6], [0, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>svm_w1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>[[2, 7], [0, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>svm_w1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>[[2, 7], [0, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>logreg_pca_w1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>[[2, 7], [0, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>xgboost_w1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>[[2, 7], [0, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>svm_w1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.700</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>[[2, 7], [0, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>svm_w1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>[[4, 5], [1, 5]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>svm_w1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.900</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>[[1, 8], [0, 6]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            model  svd_dim  window  threshold  accuracy  precision    recall  \\\n",
       "2       logreg_w1       50       1      0.450  0.800000   0.666667  1.000000   \n",
       "56  logreg_pca_w1       50       1      0.450  0.866667   1.000000  0.666667   \n",
       "55  logreg_pca_w1       50       1      0.425  0.800000   0.714286  0.833333   \n",
       "3       logreg_w1       50       1      0.475  0.800000   0.800000  0.666667   \n",
       "40         svm_w1       50       1     -0.200  0.733333   0.625000  0.833333   \n",
       "57  logreg_pca_w1       50       1      0.475  0.800000   1.000000  0.500000   \n",
       "39         svm_w1       50       1     -0.300  0.666667   0.555556  0.833333   \n",
       "1       logreg_w1       50       1      0.425  0.600000   0.500000  1.000000   \n",
       "36         svm_w1       50       1     -0.600  0.533333   0.461538  1.000000   \n",
       "34         svm_w1       50       1     -0.800  0.533333   0.461538  1.000000   \n",
       "54  logreg_pca_w1       50       1      0.400  0.533333   0.461538  1.000000   \n",
       "18     xgboost_w1       50       1      0.400  0.533333   0.461538  1.000000   \n",
       "35         svm_w1       50       1     -0.700  0.533333   0.461538  1.000000   \n",
       "38         svm_w1       50       1     -0.400  0.600000   0.500000  0.833333   \n",
       "33         svm_w1       50       1     -0.900  0.466667   0.428571  1.000000   \n",
       "\n",
       "          f1         confusion  \n",
       "2   0.800000  [[6, 3], [0, 6]]  \n",
       "56  0.800000  [[9, 0], [2, 4]]  \n",
       "55  0.769231  [[7, 2], [1, 5]]  \n",
       "3   0.727273  [[8, 1], [2, 4]]  \n",
       "40  0.714286  [[6, 3], [1, 5]]  \n",
       "57  0.666667  [[9, 0], [3, 3]]  \n",
       "39  0.666667  [[5, 4], [1, 5]]  \n",
       "1   0.666667  [[3, 6], [0, 6]]  \n",
       "36  0.631579  [[2, 7], [0, 6]]  \n",
       "34  0.631579  [[2, 7], [0, 6]]  \n",
       "54  0.631579  [[2, 7], [0, 6]]  \n",
       "18  0.631579  [[2, 7], [0, 6]]  \n",
       "35  0.631579  [[2, 7], [0, 6]]  \n",
       "38  0.625000  [[4, 5], [1, 5]]  \n",
       "33  0.600000  [[1, 8], [0, 6]]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results\n",
    "results_df = pd.DataFrame(master_results)\n",
    "results_df_sorted = results_df.sort_values(\"f1\", ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 model threshold combinations:\")\n",
    "results_df_sorted.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cc0f496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best configuration per model:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>svd_dim</th>\n",
       "      <th>window</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>confusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logreg_w1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>[[6, 3], [0, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logreg_pca_w1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>[[9, 0], [2, 4]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>svm_w1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>[[6, 3], [1, 5]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xgboost_w1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>[[2, 7], [0, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>logreg_pca_w3</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>[[1, 8], [0, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logreg_w3</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>[[1, 8], [0, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>svm_w3</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>[[3, 6], [1, 5]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xgboost_w3</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>[[3, 6], [3, 3]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model  svd_dim  window  threshold  accuracy  precision    recall  \\\n",
       "0      logreg_w1       50       1       0.45  0.800000   0.666667  1.000000   \n",
       "1  logreg_pca_w1       50       1       0.45  0.866667   1.000000  0.666667   \n",
       "2         svm_w1       50       1      -0.20  0.733333   0.625000  0.833333   \n",
       "3     xgboost_w1       50       1       0.40  0.533333   0.461538  1.000000   \n",
       "4  logreg_pca_w3       50       3       0.40  0.466667   0.428571  1.000000   \n",
       "5      logreg_w3       50       3       0.40  0.466667   0.428571  1.000000   \n",
       "6         svm_w3       50       3      -0.30  0.533333   0.454545  0.833333   \n",
       "7     xgboost_w3       50       3       0.40  0.400000   0.333333  0.500000   \n",
       "\n",
       "         f1         confusion  \n",
       "0  0.800000  [[6, 3], [0, 6]]  \n",
       "1  0.800000  [[9, 0], [2, 4]]  \n",
       "2  0.714286  [[6, 3], [1, 5]]  \n",
       "3  0.631579  [[2, 7], [0, 6]]  \n",
       "4  0.600000  [[1, 8], [0, 6]]  \n",
       "5  0.600000  [[1, 8], [0, 6]]  \n",
       "6  0.588235  [[3, 6], [1, 5]]  \n",
       "7  0.400000  [[3, 6], [3, 3]]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best config per model\n",
    "best_by_model = (\n",
    "    results_df\n",
    "    .sort_values(\"f1\", ascending=False)\n",
    "    .groupby(\"model\")\n",
    "    .head(1)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"\\nBest configuration per model:\")\n",
    "best_by_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24f5133",
   "metadata": {},
   "source": [
    "# Interpretation\n",
    "\n",
    "Overall, logistic regression with a window size of 1 (both with and without pca) performed the best, achieving an f1 score of 0.80, differing only in their precision and recall. This indicates that:\n",
    "- although both models share the same F1 score, the svd + pca version achieves higher accuracy because it makes fewer positive predictions (higher precision), whereas the svd-only version achieves perfect recall by predicting ‘up’ more frequently\n",
    "- the relationship between svd-encoded headlines and previous day return is approximately linear to next day direction\n",
    "- the dataset size of ~224 training days is way too small for a complex model like xgboost to generalize accurately\n",
    "\n",
    "We also see that window size 1 outperforms window size 3 consistently across all three models. This may by due to:\n",
    "- financial return autocorrelation decays extremely fast, so including return of 2 or 3 days prior adds more noise than useful signals (especially in small datasets)\n",
    "- window size 3 reduces the number of training samples (not by a lot), and adds unnecessary dimensionality\\\n",
    "This suggest shallow momentum in nvidia's short term returns\n",
    "\n",
    "Logistic Regression:\n",
    "- achieved the best f1 scores for both svd and pca version indicating that it benefits from a smooth linear decision boundary, proper scaling of return features, and the geometry of svd embeddings, which create linearly separable representations\n",
    "- logisitc regression's performance also suggests that the predicitve structure of our data is close to linear\n",
    "\n",
    "Linear SVM:\n",
    "- selected small C values indicating heavy regularization indiciating high noise in the features\n",
    "- performs well (f1 = 0.73) but not as well as logistic regression\n",
    "- margins become unstable with small datasets, leading to more sensitive support vector placements than logistic regression\n",
    "- reinforces that simple linear models behave best in low sample and low signal data\n",
    "\n",
    "XGBoost:\n",
    "- worst of the three models\n",
    "- poor training and generalization due to small dataset\n",
    "- dense svd features removed sparsity, which tree models normally exploit\n",
    "- parameters also indcate overfitting with low learning rate (0.01), shallow trees (depth of 2 or 3), and many trees (100-200). these are classic signs that the model is attempting not to overfit but failing to extract meaningful structure\\\n",
    "XGBoost is too expressive for this setup and overfits the noise and not the signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0897734",
   "metadata": {},
   "source": [
    "## Design choices\n",
    "Why pca after svd and why only for logistic regression? Why not pca directly on tfidf?\n",
    "- tfidf is very high dimensional and sparse\n",
    "- pca requires computing a dense covariance matrix, which is computationally expensive\n",
    "\n",
    "Why pca after svd?\n",
    "- svd compresses tfidf into 50 dimensional dense representations, which pca optimally reduces to 10\n",
    "- this way, we test whether logistic regression benefits from a more compact low-rank structure or not\n",
    "\n",
    "Why PCA only for Logistic Regression?\n",
    "- pca removes variance that may be useful for nonlinear models like xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307b6650",
   "metadata": {},
   "source": [
    "The modeling results imply:\n",
    "- headline news contains weak but non-zero predictive information, especially when distilled via svd\n",
    "- previous day return is the strongest consistent feature, and adding longer return windows dilutes signal\n",
    "- simple linear models generalize best\n",
    "- nonlinear learners fail due to low sample size and noisy market structure\n",
    "- dimensionality reduction is essential given raw TF-IDF is too high dimensional, but SVD captures coherent semantics\n",
    "\n",
    "Overall, the market is difficult to predict, but simple linear models combined with single-day momentum can extract the limited predictive signal present in this dataset\n",
    "\n",
    "Possible future work:\n",
    "- Add sentiment lexicon scores (e.g., Loughran–McDonald, VADER) to complement TF-IDF\n",
    "- incorporate other market features: volatility, volume, sector index movements\n",
    "- explore topic models (LDA) or transformer embeddings (FinBERT, headline embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78151257",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
