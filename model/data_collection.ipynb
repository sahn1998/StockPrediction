{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a94f98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests          # For making HTTP requests to the GDELT API\n",
    "import pandas as pd      # For handling tabular data as DataFrames\n",
    "import time              # For adding delays to avoid API rate limiting\n",
    "import json              # For safely parsing JSON responses\n",
    "\n",
    "from datetime import datetime, timedelta  # For working with dates and times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2d7aff",
   "metadata": {},
   "source": [
    "# 2. Fetch Data from GDELT API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31ff820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_gdelt_articles_safe(url):\n",
    "    \"\"\"\n",
    "    Fetch articles from a full GDELT API URL safely.\n",
    "    Applies basic validation and returns a cleaned pandas DataFrame.\n",
    "    Only English-language articles are kept.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Send a GET request to the provided URL with a 30-second timeout\n",
    "        res = requests.get(url, timeout=30)\n",
    "\n",
    "        # If the status code is not 200 (OK), stop and break. Return warning and end the running \n",
    "        if res.status_code != 200:\n",
    "            print(f\"⚠️ Status {res.status_code}, skipping.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Strip whitespace from the response text\n",
    "        text = res.text.strip()\n",
    "\n",
    "        # GDELT JSON responses should start with \"{\" and end with \"}\"\n",
    "        # If not, we consider it incomplete or invalid\n",
    "        if not text.startswith(\"{\") or not text.endswith(\"}\"):\n",
    "            print(\"⚠️ Incomplete JSON, skipping.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Parse the JSON text into a Python dictionary\n",
    "        data = json.loads(text)\n",
    "\n",
    "        df = pd.DataFrame(data[\"articles\"])\n",
    "\n",
    "        # This df should have columns of url, url_mobile, title, seendate, socialimage, domain, language, sourcecountry\n",
    "        for col in [\"title\", \"seendate\", \"url\", \"sourcecountry\", \"language\", \"domain\"]:\n",
    "            if col not in df.columns:\n",
    "                df[col] = None\n",
    "\n",
    "        # Convert 'seendate' column to datetime, coercing invalid values to NaT\n",
    "        df[\"seendate\"] = pd.to_datetime(df[\"seendate\"], errors=\"coerce\")\n",
    "\n",
    "        # Keep only rows where the source country is the United States (\"US\")\n",
    "        df = df[df[\"language\"] == \"English\"]\n",
    "\n",
    "        # Restrict the DataFrame to the columns we want to keep\n",
    "        df = df[[\"title\", \"seendate\", \"url\", \"sourcecountry\", \"language\", \"domain\"]]\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        # If any error occurs, log it and return an empty DataFrame\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a4f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_gdelt_articles_window(query, start_dt, end_dt, maxrecords=100):\n",
    "    \"\"\"\n",
    "    Fetch Tesla articles from GDELT for a single window.\n",
    "    start_dt and end_dt are datetime objects.\n",
    "    \"\"\"\n",
    "    start_str = start_dt.strftime(\"%Y%m%d%H%M%S\")\n",
    "    end_str = end_dt.strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    url = (\n",
    "        f\"https://api.gdeltproject.org/api/v2/doc/doc?\"\n",
    "        f\"query={query}&mode=ArtList&maxrecords={maxrecords}\"\n",
    "        f\"&startdatetime={start_str}&enddatetime={end_str}&format=JSON\"\n",
    "    )\n",
    "\n",
    "    return fetch_gdelt_articles_safe(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3104f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_gdelt_articles_range(\n",
    "    query,\n",
    "    start_date,\n",
    "    end_date,\n",
    "    step_days=1,\n",
    "    maxrecords=100\n",
    "):\n",
    "    \"\"\"\n",
    "    Collect GDELT articles for a given query over a date range.\n",
    "    The range is split into windows of 'step_days' days.\n",
    "    Only English-language articles from US sources are retained.\n",
    "    \"\"\"\n",
    "\n",
    "    # If end_date is not provided, default to today's date (YYYY-MM-DD)\n",
    "    if end_date is None:\n",
    "        end_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Store all window DataFrames in a list\n",
    "    all_articles = []\n",
    "\n",
    "    # Convert input date strings to datetime objects\n",
    "    current_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_dt = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "\n",
    "    # Loop over the date range in chunks of 'step_days'\n",
    "    while current_dt < end_dt:\n",
    "        # Compute the end of the current window\n",
    "        next_dt = current_dt + timedelta(days=step_days)\n",
    "\n",
    "        # Log which window we are fetching\n",
    "        print(\n",
    "            f\"Fetching articles from \"\n",
    "            f\"{current_dt.strftime('%Y-%m-%d')} \"\n",
    "            f\"to {next_dt.strftime('%Y-%m-%d')}...\"\n",
    "        )\n",
    "\n",
    "        # Fetch articles for this window\n",
    "        df = fetch_gdelt_articles_window(\n",
    "            query=query,\n",
    "            start_dt=current_dt,\n",
    "            end_dt=next_dt,\n",
    "            maxrecords=maxrecords\n",
    "        )\n",
    "\n",
    "        # If we got any articles, append them to the list\n",
    "        if not df.empty:\n",
    "            all_articles.append(df)\n",
    "            print(f\"✅ {len(df)} articles collected in this window.\")\n",
    "        else:\n",
    "            print(\"⚠️ No articles found in this window.\")\n",
    "\n",
    "        # Move to the next window\n",
    "        current_dt = next_dt\n",
    "\n",
    "        # Sleep to avoid hitting GDELT rate limits\n",
    "        time.sleep(60)\n",
    "\n",
    "    # After looping through all windows, combine results if any were found\n",
    "    if all_articles:\n",
    "        # Concatenate all DataFrames, drop duplicate URLs, and sort by date\n",
    "        combined_df = (\n",
    "            pd.concat(all_articles, ignore_index=True)\n",
    "              .drop_duplicates(subset=\"url\")\n",
    "        )\n",
    "        combined_df.sort_values(\"seendate\", inplace=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        # If no articles were collected at all, log and return empty DataFrame\n",
    "        print(\"⚠️ No articles collected at all.\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b769af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching articles from 2024-11-01 to 2024-11-08...\n",
      "✅ 150 articles collected in this window.\n",
      "Fetching articles from 2024-11-08 to 2024-11-15...\n",
      "✅ 198 articles collected in this window.\n",
      "Fetching articles from 2024-11-15 to 2024-11-22...\n",
      "✅ 250 articles collected in this window.\n",
      "Fetching articles from 2024-11-22 to 2024-11-29...\n",
      "✅ 220 articles collected in this window.\n",
      "Fetching articles from 2024-11-29 to 2024-12-06...\n",
      "✅ 180 articles collected in this window.\n",
      "Fetching articles from 2024-12-06 to 2024-12-13...\n",
      "✅ 206 articles collected in this window.\n",
      "Fetching articles from 2024-12-13 to 2024-12-20...\n",
      "✅ 230 articles collected in this window.\n",
      "Fetching articles from 2024-12-20 to 2024-12-27...\n",
      "✅ 140 articles collected in this window.\n",
      "Fetching articles from 2024-12-27 to 2025-01-03...\n",
      "✅ 175 articles collected in this window.\n",
      "Fetching articles from 2025-01-03 to 2025-01-10...\n",
      "✅ 250 articles collected in this window.\n",
      "Fetching articles from 2025-01-10 to 2025-01-17...\n",
      "✅ 217 articles collected in this window.\n",
      "Fetching articles from 2025-01-17 to 2025-01-24...\n",
      "✅ 186 articles collected in this window.\n",
      "Fetching articles from 2025-01-24 to 2025-01-31...\n",
      "✅ 250 articles collected in this window.\n",
      "Fetching articles from 2025-01-31 to 2025-02-07...\n",
      "✅ 250 articles collected in this window.\n",
      "Fetching articles from 2025-02-07 to 2025-02-14...\n",
      "✅ 181 articles collected in this window.\n",
      "Fetching articles from 2025-02-14 to 2025-02-21...\n",
      "✅ 205 articles collected in this window.\n",
      "Fetching articles from 2025-02-21 to 2025-02-28...\n",
      "✅ 250 articles collected in this window.\n",
      "Fetching articles from 2025-02-28 to 2025-03-07...\n",
      "✅ 250 articles collected in this window.\n",
      "Fetching articles from 2025-03-07 to 2025-03-14...\n",
      "✅ 225 articles collected in this window.\n",
      "Fetching articles from 2025-03-14 to 2025-03-21...\n",
      "✅ 250 articles collected in this window.\n",
      "Fetching articles from 2025-03-21 to 2025-03-28...\n",
      "✅ 250 articles collected in this window.\n",
      "Fetching articles from 2025-03-28 to 2025-04-04...\n",
      "✅ 201 articles collected in this window.\n",
      "Fetching articles from 2025-04-04 to 2025-04-11...\n",
      "✅ 238 articles collected in this window.\n",
      "Fetching articles from 2025-04-11 to 2025-04-18...\n",
      "✅ 250 articles collected in this window.\n",
      "Fetching articles from 2025-04-18 to 2025-04-25...\n",
      "✅ 197 articles collected in this window.\n",
      "Fetching articles from 2025-04-25 to 2025-05-02...\n",
      "✅ 187 articles collected in this window.\n",
      "Fetching articles from 2025-05-02 to 2025-05-09...\n",
      "✅ 164 articles collected in this window.\n",
      "Fetching articles from 2025-05-09 to 2025-05-16...\n",
      "✅ 224 articles collected in this window.\n",
      "Fetching articles from 2025-05-16 to 2025-05-23...\n",
      "✅ 180 articles collected in this window.\n",
      "Fetching articles from 2025-05-23 to 2025-05-30...\n",
      "✅ 250 articles collected in this window.\n",
      "Fetching articles from 2025-05-30 to 2025-06-06...\n",
      "✅ 250 articles collected in this window.\n",
      "Fetching articles from 2025-06-06 to 2025-06-13...\n",
      "✅ 193 articles collected in this window.\n",
      "Fetching articles from 2025-06-13 to 2025-06-20...\n",
      "✅ 35 articles collected in this window.\n",
      "Fetching articles from 2025-06-20 to 2025-06-27...\n",
      "❌ Error: 'articles'\n",
      "⚠️ No articles found in this window.\n",
      "Fetching articles from 2025-06-27 to 2025-07-04...\n",
      "✅ 84 articles collected in this window.\n",
      "Fetching articles from 2025-07-04 to 2025-07-11...\n",
      "✅ 184 articles collected in this window.\n",
      "Fetching articles from 2025-07-11 to 2025-07-18...\n",
      "✅ 250 articles collected in this window.\n",
      "Fetching articles from 2025-07-18 to 2025-07-25...\n",
      "✅ 199 articles collected in this window.\n",
      "Fetching articles from 2025-07-25 to 2025-08-01...\n",
      "✅ 247 articles collected in this window.\n",
      "Fetching articles from 2025-08-01 to 2025-08-08...\n",
      "✅ 198 articles collected in this window.\n",
      "Fetching articles from 2025-08-08 to 2025-08-15...\n",
      "✅ 239 articles collected in this window.\n",
      "Fetching articles from 2025-08-15 to 2025-08-22...\n",
      "✅ 250 articles collected in this window.\n",
      "Fetching articles from 2025-08-22 to 2025-08-29...\n",
      "✅ 250 articles collected in this window.\n",
      "Fetching articles from 2025-08-29 to 2025-09-05...\n",
      "✅ 250 articles collected in this window.\n",
      "Fetching articles from 2025-09-05 to 2025-09-12...\n",
      "✅ 250 articles collected in this window.\n",
      "Fetching articles from 2025-09-12 to 2025-09-19...\n",
      "✅ 250 articles collected in this window.\n",
      "Fetching articles from 2025-09-19 to 2025-09-26...\n",
      "✅ 250 articles collected in this window.\n",
      "Fetching articles from 2025-09-26 to 2025-10-03...\n",
      "✅ 250 articles collected in this window.\n",
      "Fetching articles from 2025-10-03 to 2025-10-10...\n",
      "✅ 250 articles collected in this window.\n",
      "Fetching articles from 2025-10-10 to 2025-10-17...\n",
      "✅ 250 articles collected in this window.\n",
      "Fetching articles from 2025-10-17 to 2025-10-24...\n",
      "✅ 223 articles collected in this window.\n",
      "Fetching articles from 2025-10-24 to 2025-10-31...\n",
      "✅ 250 articles collected in this window.\n",
      "Fetching articles from 2025-10-31 to 2025-11-07...\n",
      "✅ 250 articles collected in this window.\n",
      "\n",
      "Total articles collected: 11197\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>seendate</th>\n",
       "      <th>url</th>\n",
       "      <th>sourcecountry</th>\n",
       "      <th>language</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Why Nvidia stock is sinking today</td>\n",
       "      <td>2024-11-01 01:30:00+00:00</td>\n",
       "      <td>https://www.fool.com.au/2024/11/01/why-nvidia-...</td>\n",
       "      <td>Australia</td>\n",
       "      <td>English</td>\n",
       "      <td>fool.com.au</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Stock market suffers a Halloween selloff as te...</td>\n",
       "      <td>2024-11-01 01:30:00+00:00</td>\n",
       "      <td>https://www.morningstar.com/news/marketwatch/2...</td>\n",
       "      <td>China</td>\n",
       "      <td>English</td>\n",
       "      <td>morningstar.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>US close : Stocks sharply lower following tech...</td>\n",
       "      <td>2024-11-01 01:45:00+00:00</td>\n",
       "      <td>https://www.sharecast.com/news/market-report-u...</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>English</td>\n",
       "      <td>sharecast.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Why Nvidia Stock Is Sinking Today | The Motley...</td>\n",
       "      <td>2024-11-01 02:15:00+00:00</td>\n",
       "      <td>https://www.fool.com/investing/2024/10/31/why-...</td>\n",
       "      <td>United States</td>\n",
       "      <td>English</td>\n",
       "      <td>fool.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Billionaire Philippe Laffont of Coatue Is Dump...</td>\n",
       "      <td>2024-11-01 03:00:00+00:00</td>\n",
       "      <td>https://finance.yahoo.com/news/billionaire-phi...</td>\n",
       "      <td>United States</td>\n",
       "      <td>English</td>\n",
       "      <td>finance.yahoo.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "43                   Why Nvidia stock is sinking today   \n",
       "126  Stock market suffers a Halloween selloff as te...   \n",
       "145  US close : Stocks sharply lower following tech...   \n",
       "30   Why Nvidia Stock Is Sinking Today | The Motley...   \n",
       "105  Billionaire Philippe Laffont of Coatue Is Dump...   \n",
       "\n",
       "                     seendate  \\\n",
       "43  2024-11-01 01:30:00+00:00   \n",
       "126 2024-11-01 01:30:00+00:00   \n",
       "145 2024-11-01 01:45:00+00:00   \n",
       "30  2024-11-01 02:15:00+00:00   \n",
       "105 2024-11-01 03:00:00+00:00   \n",
       "\n",
       "                                                   url   sourcecountry  \\\n",
       "43   https://www.fool.com.au/2024/11/01/why-nvidia-...       Australia   \n",
       "126  https://www.morningstar.com/news/marketwatch/2...           China   \n",
       "145  https://www.sharecast.com/news/market-report-u...  United Kingdom   \n",
       "30   https://www.fool.com/investing/2024/10/31/why-...   United States   \n",
       "105  https://finance.yahoo.com/news/billionaire-phi...   United States   \n",
       "\n",
       "    language             domain  \n",
       "43   English        fool.com.au  \n",
       "126  English    morningstar.com  \n",
       "145  English      sharecast.com  \n",
       "30   English           fool.com  \n",
       "105  English  finance.yahoo.com  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdelt_NVDA = fetch_gdelt_articles_range(\n",
    "    query=\"NVIDIA NVDA\",\n",
    "    start_date=\"2024-11-01\",\n",
    "    end_date=\"2025-11-01\",\n",
    "    step_days=7,\n",
    "    maxrecords=250\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal articles collected: {len(gdelt_NVDA)}\")\n",
    "gdelt_NVDA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "707c14bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdelt_NVDA.to_csv('nvda1year.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00f91e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique days: 355\n",
      "First few unique dates: [datetime.date(2024, 11, 1), datetime.date(2024, 11, 2), datetime.date(2024, 11, 3), datetime.date(2024, 11, 4), datetime.date(2024, 11, 5), datetime.date(2024, 11, 6), datetime.date(2024, 11, 7), datetime.date(2024, 11, 8), datetime.date(2024, 11, 9), datetime.date(2024, 11, 10)]\n"
     ]
    }
   ],
   "source": [
    "# assuming df is your dataframe\n",
    "gdelt_NVDA['seendate'] = pd.to_datetime(gdelt_NVDA['seendate'])  # ensure it's a datetime\n",
    "gdelt_NVDA['date'] = gdelt_NVDA['seendate'].dt.date              # extract just the date (no time)\n",
    "\n",
    "# unique days\n",
    "unique_days = gdelt_NVDA['date'].unique()\n",
    "print(f\"Total unique days: {len(unique_days)}\")\n",
    "print(\"First few unique dates:\", sorted(unique_days)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a949f776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date  article_count\n",
      "0   2024-11-01             24\n",
      "1   2024-11-02             20\n",
      "2   2024-11-03             10\n",
      "3   2024-11-04             31\n",
      "4   2024-11-05             28\n",
      "..         ...            ...\n",
      "95  2025-02-04             34\n",
      "96  2025-02-05             35\n",
      "97  2025-02-06             41\n",
      "98  2025-02-07             22\n",
      "99  2025-02-08             17\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "articles_per_day = gdelt_NVDA.groupby('date').size().reset_index(name='article_count')\n",
    "\n",
    "# sort by date\n",
    "articles_per_day = articles_per_day.sort_values('date')\n",
    "\n",
    "# show top few\n",
    "print(articles_per_day.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9dce51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
